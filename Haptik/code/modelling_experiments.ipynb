{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-moore",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. BOW based classification models\n",
    "2. Word embedding models which can use subword information (pre-trained and fine-tuned)\n",
    "3. Sentence embedding models (pre-trained and fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "completed-perfume",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varunnathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varunnathan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/varunnathan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunnathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/varunnathan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, swifter, re\n",
    "from constants import *\n",
    "from utility import *\n",
    "from preprocess_utils import _remove_non_ascii_characters\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boolean-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "DEFAULT_CLASS = 'NO_NODES_DETECTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opponent-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.48 ms, sys: 1.29 ms, total: 4.77 ms\n",
      "Wall time: 5.83 ms\n",
      "CPU times: user 2.41 ms, sys: 2.7 ms, total: 5.11 ms\n",
      "Wall time: 7.08 ms\n",
      "(324, 2) \t (394, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "%time df_train = pd.read_csv(TRAIN_FN)\n",
    "%time df_test = pd.read_csv(TEST_FN)\n",
    "\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_train.shape, '\\t', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outer-share",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentence    0\n",
       " label       0\n",
       " dtype: int64,\n",
       " sentence    0\n",
       " label       0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum(), df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-discount",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Contraction\n",
    "2. Lower casing\n",
    "3. Remove non-alphabets\n",
    "4. Remove stop words\n",
    "5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "small-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Text_Preprocessing(keep_eng=False, remove_nonalpha=False, lower_case=True,\n",
    "                         remove_punkt=False, remove_stop=True, remove_numerals=False,\n",
    "                         spell_check=False, contraction=True,\n",
    "                         contraction_var=CONTRACTIONS, stem=True,\n",
    "                         lem=False, filter_pos=False, pos_var=('N', 'J'),\n",
    "                         tokenize=False, template_removal=False,\n",
    "                         template_start_string='', regex_cleaning=False,\n",
    "                         remove_ignore_words=False, ignore_words=IGNORE_WORDS,\n",
    "                         custom_stoplist=[], word_size=2, word_size_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decreased-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n",
      "remove stop words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7952f9bf3e470fa8ac82f48e0f3ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfaa64baa6845bb8df1c0177d5da14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n",
      "remove stop words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b74a2efcb7b4754b59dbe74bf0439a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93c6a2b2a2d4475b9d60ab3606b0dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 222 ms, sys: 22.4 ms, total: 245 ms\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['sent_pre'] = df_train['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_train['sent_pre'] = preprocess_obj.fit_transform(df_train['sent_pre'])\n",
    "df_train['sent_pre'] = df_train['sent_pre'].swifter.apply(lambda x:\n",
    "                                                          re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_test['sent_pre'] = df_test['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_test['sent_pre'] = preprocess_obj.fit_transform(df_test['sent_pre'])\n",
    "df_test['sent_pre'] = df_test['sent_pre'].swifter.apply(lambda x:\n",
    "                                                        re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_train.fillna(value={'sent_pre': ''}, inplace=True)\n",
    "df_test.fillna(value={'sent_pre': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "geographic-salvation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sent_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You guys provide EMI option?</td>\n",
       "      <td>guy provid emi option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you offer Zero Percent EMI payment options?</td>\n",
       "      <td>offer zero percent emi payment option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0% EMI.</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMI</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want in installment</td>\n",
       "      <td>want instal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I want it on 0% interest</td>\n",
       "      <td>want interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How to get in EMI</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what about emi options</td>\n",
       "      <td>emi option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I need emi payment.</td>\n",
       "      <td>need emi payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How to EMI</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence  \\\n",
       "0                    You guys provide EMI option?   \n",
       "1  Do you offer Zero Percent EMI payment options?   \n",
       "2                                         0% EMI.   \n",
       "3                                             EMI   \n",
       "4                           I want in installment   \n",
       "5                        I want it on 0% interest   \n",
       "6                               How to get in EMI   \n",
       "7                          what about emi options   \n",
       "8                            I need emi payment.    \n",
       "9                                      How to EMI   \n",
       "\n",
       "                                 sent_pre  \n",
       "0                  guy provid emi option   \n",
       "1  offer zero percent emi payment option   \n",
       "2                                    emi   \n",
       "3                                     emi  \n",
       "4                             want instal  \n",
       "5                           want interest  \n",
       "6                                     emi  \n",
       "7                              emi option  \n",
       "8                       need emi payment   \n",
       "9                                     emi  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['sentence', 'sent_pre']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "norwegian-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df_train['label'].tolist() + df_test['label'].tolist())\n",
    "le = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "le.fit(labels)\n",
    "df_train['label_trans'] = le.transform(df_train['label'].values)\n",
    "df_test['label_trans'] = le.transform(df_test['label'].values)\n",
    "labels_enc = np.array(df_train['label_trans'].tolist() + df_test['label_trans'].tolist())\n",
    "labels_enc = labels_enc.reshape(len(labels_enc), 1)\n",
    "onehot_encoder.fit(labels_enc)\n",
    "train_labels_ohe = onehot_encoder.transform(df_train['label_trans'].values.reshape(-1, 1))\n",
    "test_labels_ohe = onehot_encoder.transform(df_test['label_trans'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "greater-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label_trans</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100_NIGHT_TRIAL_OFFER</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABOUT_SOF_MATTRESS</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCEL_ORDER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHECK_PINCODE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMPARISON</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAY_IN_DELIVERY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTRIBUTORS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMI</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERGO_FEATURES</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEAD_GEN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATTRESS_COST</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO_NODES_DETECTED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFFERS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDER_STATUS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORTHO_FEATURES</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PILLOWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRODUCT_VARIANTS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RETURN_EXCHANGE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIZE_CUSTOMIZATION</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHAT_SIZE_TO_ORDER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "label_trans            0   1   2   3   4   5   6   7   8   9   ...  11   12  \\\n",
       "label                                                          ...            \n",
       "100_NIGHT_TRIAL_OFFER   5   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "ABOUT_SOF_MATTRESS      0   2   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "CANCEL_ORDER            0   0   6   0   0   0   0   0   0   0  ...   0    0   \n",
       "CHECK_PINCODE           0   0   0  22   0   0   0   0   0   0  ...   0    0   \n",
       "COD                     0   0   0   0   8   0   0   0   0   0  ...   0    0   \n",
       "COMPARISON              0   0   0   0   0  18   0   0   0   0  ...   0    0   \n",
       "DELAY_IN_DELIVERY       0   0   0   0   0   0  13   0   0   0  ...   0    0   \n",
       "DISTRIBUTORS            0   0   0   0   0   0   0   7   0   0  ...   0    0   \n",
       "EMI                     0   0   0   0   0   0   0   0  16   0  ...   0    0   \n",
       "ERGO_FEATURES           0   0   0   0   0   0   0   0   0   3  ...   0    0   \n",
       "LEAD_GEN                0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "MATTRESS_COST           0   0   0   0   0   0   0   0   0   0  ...  21    0   \n",
       "NO_NODES_DETECTED       0   0   0   0   0   0   0   0   0   0  ...   0  163   \n",
       "OFFERS                  0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "ORDER_STATUS            0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "ORTHO_FEATURES          0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "PILLOWS                 0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "PRODUCT_VARIANTS        0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "RETURN_EXCHANGE         0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "SIZE_CUSTOMIZATION      0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "WHAT_SIZE_TO_ORDER      0   0   0   0   0   0   0   0   0   0  ...   0    0   \n",
       "\n",
       "label_trans            13  14  15  16  17  18  19  21  \n",
       "label                                                  \n",
       "100_NIGHT_TRIAL_OFFER   0   0   0   0   0   0   0   0  \n",
       "ABOUT_SOF_MATTRESS      0   0   0   0   0   0   0   0  \n",
       "CANCEL_ORDER            0   0   0   0   0   0   0   0  \n",
       "CHECK_PINCODE           0   0   0   0   0   0   0   0  \n",
       "COD                     0   0   0   0   0   0   0   0  \n",
       "COMPARISON              0   0   0   0   0   0   0   0  \n",
       "DELAY_IN_DELIVERY       0   0   0   0   0   0   0   0  \n",
       "DISTRIBUTORS            0   0   0   0   0   0   0   0  \n",
       "EMI                     0   0   0   0   0   0   0   0  \n",
       "ERGO_FEATURES           0   0   0   0   0   0   0   0  \n",
       "LEAD_GEN                0   0   0   0   0   0   0   0  \n",
       "MATTRESS_COST           0   0   0   0   0   0   0   0  \n",
       "NO_NODES_DETECTED       0   0   0   0   0   0   0   0  \n",
       "OFFERS                  9   0   0   0   0   0   0   0  \n",
       "ORDER_STATUS            0   8   0   0   0   0   0   0  \n",
       "ORTHO_FEATURES          0   0  11   0   0   0   0   0  \n",
       "PILLOWS                 0   0   0  13   0   0   0   0  \n",
       "PRODUCT_VARIANTS        0   0   0   0   8   0   0   0  \n",
       "RETURN_EXCHANGE         0   0   0   0   0  12   0   0  \n",
       "SIZE_CUSTOMIZATION      0   0   0   0   0   0  24   0  \n",
       "WHAT_SIZE_TO_ORDER      0   0   0   0   0   0   0   9  \n",
       "\n",
       "[21 rows x 21 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_test['label'], df_test['label_trans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tired-hours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    8\n",
       " 1    8\n",
       " 2    8\n",
       " 3    8\n",
       " 4    8\n",
       " 5    8\n",
       " 6    8\n",
       " 7    8\n",
       " 8    8\n",
       " 9    8\n",
       " Name: label_trans, dtype: int64,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label_trans'].head(10), train_labels_ohe[:10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-glass",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "historical-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "conventional-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prec_rec(y_true_ohe, pred_prob):\n",
    "    n_classes = len(set(y_true_ohe))\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    fpr, tpr = {}, {}\n",
    "    roc_auc = {}\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_ohe[:, i],\n",
    "                                                            pred_prob[:, i])\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_ohe[:, i], pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true_ohe.ravel(),\n",
    "                                                                    pred_prob.ravel())\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_ohe.ravel(), pred_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    return precision, recall, roc_auc\n",
    "\n",
    "\n",
    "def get_accuracy(y_true, pred):\n",
    "    return np.mean(y_true == pred)\n",
    "\n",
    "\n",
    "def adjust_pred_prob(pred_probs, default_class_idx):\n",
    "    out = []\n",
    "    for pred_prob in pred_probs:\n",
    "        v = max(1 - pred_prob.sum(), 0)\n",
    "        pred_prob_n = np.insert(pred_prob, default_class_idx, v)\n",
    "        out.append(pred_prob_n)\n",
    "    return np.array(out)\n",
    "    \n",
    "    \n",
    "def create_pipeline(x_train, y_train, x_test, y_test, max_df=0.5, max_features=10000,\n",
    "                    min_df=20, ngram_range=(1, 3), default_class_idx=12,\n",
    "                    class_algo='lr', **params):\n",
    "    \n",
    "    if class_algo == 'lr':\n",
    "        obj = LogisticRegression(penalty=params['penalty'], C=params['C'], random_state=100,\n",
    "                                 verbose=1, l1_ratio=params['l1_ratio'], solver='saga')\n",
    "    elif class_algo == 'gbc':\n",
    "        obj = GradientBoostingClassifier(learning_rate=params['lr'],\n",
    "                                         n_estimators=params['n_trees'],\n",
    "                                         subsample=0.9, max_depth=params['max_depth'],\n",
    "                                         random_state=100, max_features='sqrt', verbose=1)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                              min_df=min_df, ngram_range=ngram_range)),\n",
    "    ('normalize', Normalizer(copy=False)),\n",
    "    (class_algo, obj)])\n",
    "\n",
    "    start = time.time()\n",
    "    print('fitting begins\\n')\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    print('time taken: %0.f' % (time.time() - start))\n",
    "\n",
    "    print('prediction on train and test\\n')\n",
    "    pred_train = pipeline.predict(x_train)\n",
    "    pred_prob_train = pipeline.predict_proba(x_train)\n",
    "    pred_prob_train = adjust_pred_prob(pred_prob_train, default_class_idx)\n",
    "    pred_test = pipeline.predict(x_test)\n",
    "    pred_prob_test = pipeline.predict_proba(x_test)\n",
    "    pred_prob_test = adjust_pred_prob(pred_prob_test, default_class_idx)\n",
    "\n",
    "    print('shape of transformed train: ', pred_train.shape)\n",
    "    print('shape of transformed test: ', pred_test.shape)\n",
    "\n",
    "    print('Record pipeline metrics\\n')\n",
    "    tfidf = pipeline.steps[0][1]\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "\n",
    "    return pred_train, pred_prob_train, pred_test, pred_prob_test, feature_names, pipeline\n",
    "\n",
    "\n",
    "def calc_prediction(threshold, y_pred, y_pred_prob, default_class_idx):\n",
    "    probs = [y_pred_prob[i, pred] for i, pred in enumerate(y_pred)]\n",
    "    preds = [y_pred[i] if prob >= threshold else default_class_idx\n",
    "             for i, prob in enumerate(probs)]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_accuracy_per_threshold(threshold, y_true, y_pred, y_pred_prob, default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    preds = calc_prediction(threshold, y_pred, y_pred_prob, default_class_idx)\n",
    "    acc = get_accuracy(y_true, preds)\n",
    "    pred_series = pd.Series(preds)\n",
    "    mask = y_true_series == default_class_idx\n",
    "    in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "    out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "    d = {'threshold': threshold, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "         'out_scope_acc': out_scope_acc}\n",
    "    return d\n",
    "\n",
    "\n",
    "def find_optimal_threshold(thresholds, y_true, y_pred, y_pred_prob, default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    \n",
    "    out = []\n",
    "    for thresh in thresholds:\n",
    "        preds = calc_prediction(thresh, y_pred, y_pred_prob, default_class_idx)\n",
    "        acc = get_accuracy(y_true, preds)\n",
    "        pred_series = pd.Series(preds)\n",
    "        mask = y_true_series == default_class_idx\n",
    "        in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "        out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "        d = {'threshold': thresh, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "             'out_scope_acc': out_scope_acc}\n",
    "        out.append(d)\n",
    "    out = pd.DataFrame(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def summarize_results(thresh_df, min_in_scope_acc_frac=0.95):\n",
    "    max_acc = thresh_df['acc'].max()\n",
    "    max_in_scope_acc = thresh_df['in_scope_acc'].max()\n",
    "    max_out_scope_acc = thresh_df['out_scope_acc'].max()\n",
    "    min_in_scope_acc = min_in_scope_acc_frac * max_in_scope_acc\n",
    "    mask = thresh_df['in_scope_acc'] >= min_in_scope_acc\n",
    "    best_out_scope_acc = thresh_df.loc[mask, 'out_scope_acc'].max()\n",
    "    mask1 = thresh_df['out_scope_acc'] == best_out_scope_acc\n",
    "    best_acc = thresh_df.loc[mask&mask1, 'acc'].values[0]\n",
    "    best_in_scope_acc = thresh_df.loc[mask&mask1, 'in_scope_acc'].values[0]\n",
    "    best_thresh = thresh_df.loc[mask&mask1, 'threshold'].values[0]\n",
    "    return {'max_acc': max_acc, 'max_in_scope_acc': max_in_scope_acc,\n",
    "            'max_out_scope_acc': max_out_scope_acc, 'best_out_scope_acc': best_out_scope_acc,\n",
    "            'best_acc': best_acc, 'best_in_scope_acc': best_in_scope_acc,\n",
    "            'best_thresh': best_thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fatal-quantum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf + GBC\n",
      "\n",
      "fitting begins\n",
      "\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           2.5465           0.2422            3.50s\n",
      "         2           2.2573           0.2565            3.63s\n",
      "         3           2.0633           0.1201            3.51s\n",
      "         4           1.9426           0.0683            3.39s\n",
      "         5           1.8869           0.1169            3.29s\n",
      "         6           1.7367           0.1532            3.19s\n",
      "         7           1.6220           0.0632            3.07s\n",
      "         8           1.6048           0.0303            2.97s\n",
      "         9           1.5064           0.0539            2.91s\n",
      "        10           1.4701           0.0078            2.84s\n",
      "        20           1.0531           0.0847            2.53s\n",
      "        30           0.8459           0.0034            2.27s\n",
      "        40           0.6626           0.0022            2.11s\n",
      "        50           0.5635           0.0041            1.98s\n",
      "        60           0.4855          -0.0014            1.85s\n",
      "        70           0.4462           0.0018            1.71s\n",
      "        80           0.3980          -0.0045            1.56s\n",
      "        90           0.3499          -0.0026            1.42s\n",
      "       100           0.3294          -0.0001            1.30s\n",
      "       200           0.2056          -0.0046            0.00s\n",
      "time taken: 3\n",
      "prediction on train and test\n",
      "\n",
      "shape of transformed train:  (324,)\n",
      "shape of transformed test:  (394,)\n",
      "Record pipeline metrics\n",
      "\n",
      "CPU times: user 2.77 s, sys: 31.2 ms, total: 2.81 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Tfidf + GBC\\n')\n",
    "x_train, y_train = df_train['sent_pre'].values, df_train['label_trans'].values\n",
    "x_test, y_test = df_test['sent_pre'].values, df_test['label_trans'].values\n",
    "default_class_idx = le.transform([DEFAULT_CLASS])[0]\n",
    "class_algo = 'gbc'\n",
    "params = {'lr': 0.09, 'n_trees': 200, 'max_depth': 2}\n",
    "pred_train, pred_prob_train, pred_test, pred_prob_test, feature_names, pipeline = create_pipeline(\n",
    "    x_train, y_train, x_test, y_test, max_df=0.8, max_features=100000, min_df=2,\n",
    "    ngram_range=(1, 3), default_class_idx=default_class_idx, class_algo=class_algo, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "given-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 1.55 ms, total: 125 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, pred_test, pred_prob_test,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "drawn-chocolate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.616751269035533,\n",
       " 'max_in_scope_acc': 0.645021645021645,\n",
       " 'max_out_scope_acc': 0.9877300613496932,\n",
       " 'best_out_scope_acc': 0.5766871165644172,\n",
       " 'best_acc': 0.5888324873096447,\n",
       " 'best_in_scope_acc': 0.5974025974025974,\n",
       " 'best_thresh': 0.28}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.92)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "educated-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.28,\n",
       " 'acc': 0.9166666666666666,\n",
       " 'in_scope_acc': 0.9166666666666666,\n",
       " 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'],\n",
    "                           y_train, pred_train, pred_prob_train, default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aquatic-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save artifacts\n",
    "BOW_PIPELINE_FN = os.path.join(INTER_DATA_DIR, \"bow_pipeline_CA{}.pkl\")\n",
    "LE_FN = os.path.join(INTER_DATA_DIR, \"label_encoder.pkl\")\n",
    "OHE_FN = os.path.join(INTER_DATA_DIR, \"one_hot_encoder.pkl\")\n",
    "pickle.dump(pipeline, open(BOW_PIPELINE_FN.format(class_algo), 'wb'))\n",
    "pickle.dump(le, open(LE_FN, 'wb'))\n",
    "pickle.dump(onehot_encoder, open(OHE_FN, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "greenhouse-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf + LR\n",
      "\n",
      "fitting begins\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 0 seconds\n",
      "time taken: 0\n",
      "prediction on train and test\n",
      "\n",
      "shape of transformed train:  (324,)\n",
      "shape of transformed test:  (394,)\n",
      "Record pipeline metrics\n",
      "\n",
      "CPU times: user 467 ms, sys: 5.55 ms, total: 473 ms\n",
      "Wall time: 469 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Tfidf + LR\\n')\n",
    "x_train, y_train = df_train['sent_pre'].values, df_train['label_trans'].values\n",
    "x_test, y_test = df_test['sent_pre'].values, df_test['label_trans'].values\n",
    "default_class_idx = le.transform([DEFAULT_CLASS])[0]\n",
    "class_algo = 'lr'\n",
    "params = {'penalty': 'elasticnet', 'C': 50, 'l1_ratio': 0.1}\n",
    "pred_train, pred_prob_train, pred_test, pred_prob_test, feature_names, pipeline = create_pipeline(\n",
    "    x_train, y_train, x_test, y_test, max_df=0.8, max_features=100000, min_df=2,\n",
    "    ngram_range=(1, 3), default_class_idx=default_class_idx, class_algo=class_algo, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "typical-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 2.49 ms, total: 126 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, pred_test, pred_prob_test,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "abstract-actor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.6065989847715736,\n",
       " 'max_in_scope_acc': 0.7012987012987013,\n",
       " 'max_out_scope_acc': 0.9815950920245399,\n",
       " 'best_out_scope_acc': 0.5030674846625767,\n",
       " 'best_acc': 0.5888324873096447,\n",
       " 'best_in_scope_acc': 0.6493506493506493,\n",
       " 'best_thresh': 0.25}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.92)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "designing-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.25,\n",
       " 'acc': 0.9166666666666666,\n",
       " 'in_scope_acc': 0.9166666666666666,\n",
       " 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'],\n",
    "                           y_train, pred_train, pred_prob_train, default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "signed-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipeline, open(BOW_PIPELINE_FN.format(class_algo), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-gender",
   "metadata": {},
   "source": [
    "## Word Embeddings (FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-mining",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1. Contraction\n",
    "2. Lower casing\n",
    "3. Remove non-alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amateur-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Text_Preprocessing(keep_eng=False, remove_nonalpha=False, lower_case=True,\n",
    "                         remove_punkt=False, remove_stop=False, remove_numerals=False,\n",
    "                         spell_check=False, contraction=True,\n",
    "                         contraction_var=CONTRACTIONS, stem=False,\n",
    "                         lem=False, filter_pos=False, pos_var=('N', 'J'),\n",
    "                         tokenize=False, template_removal=False,\n",
    "                         template_start_string='', regex_cleaning=False,\n",
    "                         remove_ignore_words=False, ignore_words=IGNORE_WORDS,\n",
    "                         custom_stoplist=[], word_size=2, word_size_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crazy-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6f982b3f704135889d5e6249a30ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b0232a58494f11a257b29ffc2cd271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.8 ms, sys: 27.3 ms, total: 110 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['sent_pre'] = df_train['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_train['sent_pre'] = preprocess_obj.fit_transform(df_train['sent_pre'])\n",
    "df_train['sent_pre'] = df_train['sent_pre'].swifter.apply(lambda x:\n",
    "                                                          re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_test['sent_pre'] = df_test['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_test['sent_pre'] = preprocess_obj.fit_transform(df_test['sent_pre'])\n",
    "df_test['sent_pre'] = df_test['sent_pre'].swifter.apply(lambda x:\n",
    "                                                        re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_train.fillna(value={'sent_pre': ''}, inplace=True)\n",
    "df_test.fillna(value={'sent_pre': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thirty-netherlands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sent_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You guys provide EMI option?</td>\n",
       "      <td>you guys provide emi option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you offer Zero Percent EMI payment options?</td>\n",
       "      <td>do you offer zero percent emi payment options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0% EMI.</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMI</td>\n",
       "      <td>emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want in installment</td>\n",
       "      <td>i want in installment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I want it on 0% interest</td>\n",
       "      <td>i want it on interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How to get in EMI</td>\n",
       "      <td>how to get in emi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what about emi options</td>\n",
       "      <td>what about emi options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I need emi payment.</td>\n",
       "      <td>i need emi payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How to EMI</td>\n",
       "      <td>how to emi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence  \\\n",
       "0                    You guys provide EMI option?   \n",
       "1  Do you offer Zero Percent EMI payment options?   \n",
       "2                                         0% EMI.   \n",
       "3                                             EMI   \n",
       "4                           I want in installment   \n",
       "5                        I want it on 0% interest   \n",
       "6                               How to get in EMI   \n",
       "7                          what about emi options   \n",
       "8                            I need emi payment.    \n",
       "9                                      How to EMI   \n",
       "\n",
       "                                         sent_pre  \n",
       "0                    you guys provide emi option   \n",
       "1  do you offer zero percent emi payment options   \n",
       "2                                            emi   \n",
       "3                                             emi  \n",
       "4                           i want in installment  \n",
       "5                           i want it on interest  \n",
       "6                               how to get in emi  \n",
       "7                          what about emi options  \n",
       "8                             i need emi payment   \n",
       "9                                      how to emi  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['sentence', 'sent_pre']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "communist-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df_train['label'].tolist() + df_test['label'].tolist())\n",
    "le = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "le.fit(labels)\n",
    "df_train['label_trans'] = le.transform(df_train['label'].values)\n",
    "df_test['label_trans'] = le.transform(df_test['label'].values)\n",
    "labels_enc = np.array(df_train['label_trans'].tolist() + df_test['label_trans'].tolist())\n",
    "labels_enc = labels_enc.reshape(len(labels_enc), 1)\n",
    "onehot_encoder.fit(labels_enc)\n",
    "train_labels_ohe = onehot_encoder.transform(df_train['label_trans'].values.reshape(-1, 1))\n",
    "test_labels_ohe = onehot_encoder.transform(df_test['label_trans'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-general",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dietary-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_PRETRAINED_FN = os.path.join(LOCAL_ROOT, \"crawl-300d-2M-subword/crawl-300d-2M-subword.vec\")\n",
    "LABEL_PREFIX = '__label__'\n",
    "FT_TRAIN_FN = os.path.join(INTER_DATA_DIR, \"ft_prepared_data_train.txt\")\n",
    "FT_TEST_FN = os.path.join(INTER_DATA_DIR, \"ft_prepared_data_test.txt\")\n",
    "FT_SCRATCH_MODEL_FN = os.path.join(INTER_DATA_DIR, \"ft_scratch_model.bin\")\n",
    "FT_FINETUNED_MODEL_FN = os.path.join(INTER_DATA_DIR, \"ft_finetuned_model.bin\")\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "human-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_ft_training(file_name, data, dv_col='FT_DV', text_col='sent_pre'):\n",
    "    fn = open(file_name, 'w')\n",
    "    for row_num, row in data.iterrows():\n",
    "        fn.write(row[dv_col] + '\\t' + row[text_col])\n",
    "        fn.write('\\n')\n",
    "    fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "narrow-impossible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for FT model training\n",
      "\n",
      "make .txt files for fasttext training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prepare data for FT model training\\n')\n",
    "df_train['FT_DV'] = df_train['label_trans'].apply(lambda x: LABEL_PREFIX+str(x))\n",
    "df_test['FT_DV'] = df_test['label_trans'].apply(lambda x: LABEL_PREFIX+str(x))\n",
    "\n",
    "print('make .txt files for fasttext training\\n')\n",
    "prepare_data_for_ft_training(FT_TRAIN_FN, df_train, dv_col='FT_DV', text_col='sent_pre')\n",
    "prepare_data_for_ft_training(FT_TEST_FN, df_test, dv_col='FT_DV', text_col='sent_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uniform-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "common-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext training...\n",
      "\n",
      "(231, 0.5670995670995671, 0.5670995670995671)\n",
      "(231, 0.2597402597402597, 0.7792207792207793)\n",
      "CPU times: user 244 ms, sys: 14.2 ms, total: 258 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('fasttext training from scratch...\\n')\n",
    "model = fasttext.train_supervised(FT_TRAIN_FN, label_prefix=LABEL_PREFIX,\n",
    "                                  lr=0.1, epoch=100, wordNgrams=1, verbose=1,\n",
    "                                  minCount=2, dim=40)\n",
    "model.save_model(FT_SCRATCH_MODEL_FN)\n",
    "print(model.test(FT_TEST_FN, k=1))\n",
    "print(model.test(FT_TEST_FN, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fresh-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction(model, text, threshold, label_prefix, default_class_idx):\n",
    "    pred, prob = model.predict(text)\n",
    "    pred = int(pred[0].split(label_prefix)[-1])\n",
    "    if prob[0] >= threshold:\n",
    "        return pred\n",
    "    else:\n",
    "        return default_class_idx\n",
    "    \n",
    "\n",
    "def calc_prediction_series(model, texts, threshold, label_prefix, default_class_idx):\n",
    "    preds = [calc_prediction(model, text, threshold, label_prefix, default_class_idx)\n",
    "             for text in texts]\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "def get_accuracy(y_true, pred):\n",
    "    return np.mean(y_true == pred)\n",
    "\n",
    "\n",
    "def get_accuracy_per_threshold(threshold, model, texts, y_true, label_prefix,\n",
    "                               default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    preds = calc_prediction_series(model, texts, threshold, label_prefix, default_class_idx)\n",
    "    acc = get_accuracy(y_true, preds)\n",
    "    pred_series = pd.Series(preds)\n",
    "    mask = y_true_series == default_class_idx\n",
    "    in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "    out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "    d = {'threshold': threshold, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "         'out_scope_acc': out_scope_acc}\n",
    "    return d\n",
    "\n",
    "\n",
    "def find_optimal_threshold(thresholds, model, texts, y_true, label_prefix, default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    \n",
    "    out = []\n",
    "    for thresh in thresholds:\n",
    "        preds = calc_prediction_series(model, texts, thresh, label_prefix,\n",
    "                                       default_class_idx)\n",
    "        acc = get_accuracy(y_true, preds)\n",
    "        pred_series = pd.Series(preds)\n",
    "        mask = y_true_series == default_class_idx\n",
    "        in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "        out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "        d = {'threshold': thresh, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "             'out_scope_acc': out_scope_acc}\n",
    "        out.append(d)\n",
    "    out = pd.DataFrame(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def summarize_results(thresh_df, min_in_scope_acc_frac=0.95):\n",
    "    max_acc = thresh_df['acc'].max()\n",
    "    max_in_scope_acc = thresh_df['in_scope_acc'].max()\n",
    "    max_out_scope_acc = thresh_df['out_scope_acc'].max()\n",
    "    min_in_scope_acc = min_in_scope_acc_frac * max_in_scope_acc\n",
    "    mask = thresh_df['in_scope_acc'] >= min_in_scope_acc\n",
    "    best_out_scope_acc = thresh_df.loc[mask, 'out_scope_acc'].max()\n",
    "    mask1 = thresh_df['out_scope_acc'] == best_out_scope_acc\n",
    "    best_acc = thresh_df.loc[mask&mask1, 'acc'].values[0]\n",
    "    best_in_scope_acc = thresh_df.loc[mask&mask1, 'in_scope_acc'].values[0]\n",
    "    best_thresh = thresh_df.loc[mask&mask1, 'threshold'].values[0]\n",
    "    return {'max_acc': max_acc, 'max_in_scope_acc': max_in_scope_acc,\n",
    "            'max_out_scope_acc': max_out_scope_acc, 'best_out_scope_acc': best_out_scope_acc,\n",
    "            'best_acc': best_acc, 'best_in_scope_acc': best_in_scope_acc,\n",
    "            'best_thresh': best_thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "marked-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 3.1 ms, total: 479 ms\n",
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "label_prefix = LABEL_PREFIX\n",
    "default_class_idx = le.transform([DEFAULT_CLASS])[0]\n",
    "texts = df_test['sent_pre'].tolist()\n",
    "y_test = df_test['label_trans'].values\n",
    "y_train = df_train['label_trans'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, model, texts, y_test, label_prefix,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "metric-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.5609137055837563,\n",
       " 'max_in_scope_acc': 0.5670995670995671,\n",
       " 'max_out_scope_acc': 0.9877300613496932,\n",
       " 'best_out_scope_acc': 0.24539877300613497,\n",
       " 'best_acc': 0.4035532994923858,\n",
       " 'best_in_scope_acc': 0.5151515151515151,\n",
       " 'best_thresh': 0.42}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.9)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "motivated-scratch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.42,\n",
       " 'acc': 0.9814814814814815,\n",
       " 'in_scope_acc': 0.9814814814814815,\n",
       " 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "texts = df_train['sent_pre'].tolist()\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], model, texts, y_train, label_prefix,\n",
    "                           default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "apart-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext finetuning...\n",
      "\n",
      "(231, 0.6406926406926406, 0.6406926406926406)\n",
      "(231, 0.2698412698412698, 0.8095238095238095)\n",
      "CPU times: user 4min, sys: 11.1 s, total: 4min 11s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('fasttext finetuning...\\n')\n",
    "model = fasttext.train_supervised(FT_TRAIN_FN, label_prefix=LABEL_PREFIX,\n",
    "                                  lr=0.1, epoch=5, wordNgrams=1, verbose=1,\n",
    "                                  pretrained_vectors=FT_PRETRAINED_FN, dim=300, minCount=2)\n",
    "model.save_model(FT_FINETUNED_MODEL_FN)\n",
    "print(model.test(FT_TEST_FN, k=1))\n",
    "print(model.test(FT_TEST_FN, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "contained-stadium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 674 ms, sys: 45.3 ms, total: 719 ms\n",
      "Wall time: 777 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "label_prefix = LABEL_PREFIX\n",
    "default_class_idx = le.transform([DEFAULT_CLASS])[0]\n",
    "texts = df_test['sent_pre'].tolist()\n",
    "y_test = df_test['label_trans'].values\n",
    "y_train = df_train['label_trans'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, model, texts, y_test, label_prefix,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "square-pierre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.5482233502538071,\n",
       " 'max_in_scope_acc': 0.6406926406926406,\n",
       " 'max_out_scope_acc': 0.8343558282208589,\n",
       " 'best_out_scope_acc': 0.1411042944785276,\n",
       " 'best_acc': 0.41624365482233505,\n",
       " 'best_in_scope_acc': 0.6103896103896104,\n",
       " 'best_thresh': 0.38}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.95)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "collectible-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.38, 'acc': 1.0, 'in_scope_acc': 1.0, 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "texts = df_train['sent_pre'].tolist()\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], model, texts, y_train, label_prefix,\n",
    "                           default_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-champagne",
   "metadata": {},
   "source": [
    "### what happens when we don't remove non-alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prostate-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n",
      "contraction\n",
      "lower case\n",
      "CPU times: user 11.7 ms, sys: 16.6 ms, total: 28.3 ms\n",
      "Wall time: 44.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['sent_pre'] = df_train['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_train['sent_pre'] = preprocess_obj.fit_transform(df_train['sent_pre'])\n",
    "df_test['sent_pre'] = df_test['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_test['sent_pre'] = preprocess_obj.fit_transform(df_test['sent_pre'])\n",
    "df_train.fillna(value={'sent_pre': ''}, inplace=True)\n",
    "df_test.fillna(value={'sent_pre': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "correct-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_TRAIN_Wnonalpha_FN = os.path.join(INTER_DATA_DIR, \"ft_prepared_data_Wnonalpha_train.txt\")\n",
    "FT_TEST_Wnonalpha_FN = os.path.join(INTER_DATA_DIR, \"ft_prepared_data_Wnonalpha_test.txt\")\n",
    "FT_FINETUNED_MODEL_Wnonalpha_FN = os.path.join(INTER_DATA_DIR, \"ft_finetuned_model_Wnonalpha.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "structural-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for FT model training with non-alphabets\n",
      "\n",
      "make .txt files for fasttext training\n",
      "\n",
      "CPU times: user 72.9 ms, sys: 7.39 ms, total: 80.3 ms\n",
      "Wall time: 86.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('prepare data for FT model training with non-alphabets\\n')\n",
    "df_train['FT_DV'] = df_train['label_trans'].apply(lambda x: LABEL_PREFIX+str(x))\n",
    "df_test['FT_DV'] = df_test['label_trans'].apply(lambda x: LABEL_PREFIX+str(x))\n",
    "\n",
    "print('make .txt files for fasttext training\\n')\n",
    "prepare_data_for_ft_training(FT_TRAIN_Wnonalpha_FN, df_train, dv_col='FT_DV',\n",
    "                             text_col='sent_pre')\n",
    "prepare_data_for_ft_training(FT_TEST_Wnonalpha_FN, df_test, dv_col='FT_DV',\n",
    "                             text_col='sent_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "filled-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext finetuning...\n",
      "\n",
      "(231, 0.5627705627705628, 0.5627705627705628)\n",
      "(231, 0.2554112554112554, 0.7662337662337663)\n",
      "CPU times: user 4min 5s, sys: 13.3 s, total: 4min 18s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('fasttext finetuning...\\n')\n",
    "model = fasttext.train_supervised(FT_TRAIN_Wnonalpha_FN, label_prefix=LABEL_PREFIX,\n",
    "                                  lr=0.1, epoch=5, wordNgrams=1, verbose=1,\n",
    "                                  pretrained_vectors=FT_PRETRAINED_FN, dim=300, minCount=2)\n",
    "model.save_model(FT_FINETUNED_MODEL_Wnonalpha_FN)\n",
    "print(model.test(FT_TEST_Wnonalpha_FN, k=1))\n",
    "print(model.test(FT_TEST_Wnonalpha_FN, k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-pierre",
   "metadata": {},
   "source": [
    "### Results are worse!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-uganda",
   "metadata": {},
   "source": [
    "## Sentence Embeddings (Transformers)\n",
    "1. Transformers on raw text\n",
    "2. Transformers on preprocessed text (contraction, lower casing, only alphabets)\n",
    "3. SentenceTransformers on raw text\n",
    "4. SentenceTransformers on preprocessed text (contraction, lower casing, only alphabets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-clerk",
   "metadata": {},
   "source": [
    "### Bert-base-uncased model finetuned on raw lower cased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "seven-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df_train['label'].tolist())\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "label2id = {k: v for v, k in enumerate(le.classes_)}\n",
    "label2id[DEFAULT_CLASS] = len(label2id)\n",
    "df_train['label_trans'] = le.transform(df_train['label'].values)\n",
    "df_test['label_trans'] = df_test['label'].apply(lambda x: label2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "attractive-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20},\n",
       " {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21},\n",
       " {'100_NIGHT_TRIAL_OFFER': 0,\n",
       "  'ABOUT_SOF_MATTRESS': 1,\n",
       "  'CANCEL_ORDER': 2,\n",
       "  'CHECK_PINCODE': 3,\n",
       "  'COD': 4,\n",
       "  'COMPARISON': 5,\n",
       "  'DELAY_IN_DELIVERY': 6,\n",
       "  'DISTRIBUTORS': 7,\n",
       "  'EMI': 8,\n",
       "  'ERGO_FEATURES': 9,\n",
       "  'LEAD_GEN': 10,\n",
       "  'MATTRESS_COST': 11,\n",
       "  'OFFERS': 12,\n",
       "  'ORDER_STATUS': 13,\n",
       "  'ORTHO_FEATURES': 14,\n",
       "  'PILLOWS': 15,\n",
       "  'PRODUCT_VARIANTS': 16,\n",
       "  'RETURN_EXCHANGE': 17,\n",
       "  'SIZE_CUSTOMIZATION': 18,\n",
       "  'WARRANTY': 19,\n",
       "  'WHAT_SIZE_TO_ORDER': 20,\n",
       "  'NO_NODES_DETECTED': 21})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train['label_trans']), set(df_test['label_trans']), label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "diagnostic-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "FT_BERT_OUTPUT_DIR = os.path.join(INTER_DATA_DIR, \"ft_bert_output\")\n",
    "FT_BERT_LOGS_DIR = os.path.join(INTER_DATA_DIR, \"ft_bert_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "polyphonic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['sentence'].str.lower().tolist()\n",
    "train_labels = df_train['label_trans'].tolist()\n",
    "test_texts = df_test['sentence'].str.lower().tolist()\n",
    "test_labels = df_test['label_trans'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "pregnant-influence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import train_test_split\\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\\n    train_texts, train_labels, test_size=0.1, random_state=SEED, shuffle=True,\\n    stratify=train_labels)\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1, random_state=SEED, shuffle=True,\n",
    "    stratify=train_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "alternate-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 21 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(set(train_labels)),\n",
    "      set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "suited-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "romance-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 25.5 ms, total: 148 ms\n",
      "Wall time: 26.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "attempted-seating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', classes=list(set(train_labels)),\n",
    "                               y=train_labels).tolist()\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "alpha-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, class_weights=None, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.class_weights = class_weights\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        if self.class_weights:\n",
    "            item['class_weights'] = torch.tensor(self.class_weights)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = IntentDataset(train_encodings, weights, train_labels)\n",
    "val_dataset = IntentDataset(encodings=val_encodings, labels=train_labels)\n",
    "test_dataset = IntentDataset(encodings=test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "killing-analyst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['labels'].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "vertical-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "class MultiClassTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        class_weights = inputs.pop(\"class_weights\")[0]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        #print(logits.shape, labels.shape, class_weights.shape)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "refined-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2050' max='2050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2050/2050 1:34:21, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.949153</td>\n",
       "      <td>2.572654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.292687</td>\n",
       "      <td>1.747429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.475707</td>\n",
       "      <td>0.906480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.720295</td>\n",
       "      <td>0.359967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.295399</td>\n",
       "      <td>0.124002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.105778</td>\n",
       "      <td>0.038920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022112</td>\n",
       "      <td>0.012642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.009445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>0.007512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.006257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>0.004676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.004147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.003717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.003372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.003091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.002646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.002460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.002315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.002176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.001872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.001503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.001429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.001401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 27min 53s, sys: 6min 15s, total: 1h 34min 8s\n",
      "Wall time: 1h 34min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2050, training_loss=0.1960051299304497)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FT_BERT_OUTPUT_DIR,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=FT_BERT_LOGS_DIR,\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=0.00004,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluate_during_training=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(set(train_labels)))\n",
    "\n",
    "trainer = MultiClassTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cardiovascular-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(FT_BERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "organized-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction...\n",
      "CPU times: user 110 ms, sys: 225 ms, total: 335 ms\n",
      "Wall time: 577 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prediction...\")\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained(FT_BERT_OUTPUT_DIR)\n",
    "test_trainer = Trainer(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "palestinian-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def get_predictions(trainer, dataset):\n",
    "    pred, _, _ = trainer.predict(dataset)\n",
    "    preds = np.argmax(pred, axis=1)\n",
    "    probs = np.array([softmax(v)[preds[i]] for i, v in enumerate(pred)])\n",
    "    return preds, probs\n",
    "\n",
    "\n",
    "def get_threshold_based_predictions(preds, probs, threshold, default_class_idx):\n",
    "    preds_n = np.array([pred if probs[i] >= threshold else default_class_idx\n",
    "                        for i, pred in enumerate(preds)])\n",
    "    return preds_n\n",
    "\n",
    "\n",
    "def get_accuracy(y_true, pred):\n",
    "    return np.mean(y_true == pred)\n",
    "\n",
    "\n",
    "def get_accuracy_per_threshold(threshold, y_true, preds, probs, default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    preds = get_threshold_based_predictions(preds, probs, threshold, default_class_idx)\n",
    "    acc = get_accuracy(y_true, preds)\n",
    "    pred_series = pd.Series(preds)\n",
    "    mask = y_true_series == default_class_idx\n",
    "    in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "    out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "    d = {'threshold': threshold, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "         'out_scope_acc': out_scope_acc}\n",
    "    return d\n",
    "\n",
    "\n",
    "def find_optimal_threshold(thresholds, y_true, preds, probs, default_class_idx):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    \n",
    "    out = []\n",
    "    for thresh in thresholds:\n",
    "        preds = get_threshold_based_predictions(preds, probs, thresh, default_class_idx)\n",
    "        acc = get_accuracy(y_true, preds)\n",
    "        pred_series = pd.Series(preds)\n",
    "        mask = y_true_series == default_class_idx\n",
    "        in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "        out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "        d = {'threshold': thresh, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "             'out_scope_acc': out_scope_acc}\n",
    "        out.append(d)\n",
    "    out = pd.DataFrame(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def summarize_results(thresh_df, min_in_scope_acc_frac=0.95):\n",
    "    max_acc = thresh_df['acc'].max()\n",
    "    max_in_scope_acc = thresh_df['in_scope_acc'].max()\n",
    "    max_out_scope_acc = thresh_df['out_scope_acc'].max()\n",
    "    min_in_scope_acc = min_in_scope_acc_frac * max_in_scope_acc\n",
    "    mask = thresh_df['in_scope_acc'] >= min_in_scope_acc\n",
    "    best_out_scope_acc = thresh_df.loc[mask, 'out_scope_acc'].max()\n",
    "    mask1 = thresh_df['out_scope_acc'] == best_out_scope_acc\n",
    "    best_acc = thresh_df.loc[mask&mask1, 'acc'].values[0]\n",
    "    best_in_scope_acc = thresh_df.loc[mask&mask1, 'in_scope_acc'].values[0]\n",
    "    best_thresh = thresh_df.loc[mask&mask1, 'threshold'].values[0]\n",
    "    return {'max_acc': max_acc, 'max_in_scope_acc': max_in_scope_acc,\n",
    "            'max_out_scope_acc': max_out_scope_acc, 'best_out_scope_acc': best_out_scope_acc,\n",
    "            'best_acc': best_acc, 'best_in_scope_acc': best_in_scope_acc,\n",
    "            'best_thresh': best_thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "outside-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='91' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41/41 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 616 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_texts = df_train['sentence'].tolist()\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "train_dataset = IntentDataset(train_encodings)\n",
    "train_pred, train_pred_prob = get_predictions(test_trainer, train_dataset)\n",
    "test_pred, test_pred_prob = get_predictions(test_trainer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "careful-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([16, 16,  5, 17, 17,  5,  7,  5,  6, 15]),\n",
       " array([0.7652197 , 0.31089705, 0.998385  , 0.96910644, 0.9736631 ,\n",
       "        0.99834   , 0.92975086, 0.9984188 , 0.95615286, 0.4055701 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[:10], test_pred_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "amateur-vaccine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "CPU times: user 164 ms, sys: 24.9 ms, total: 189 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "default_class_idx = label2id[DEFAULT_CLASS]\n",
    "print(default_class_idx)\n",
    "y_test = df_test['label_trans'].values\n",
    "y_train = df_train['label_trans'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, test_pred, test_pred_prob,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bigger-telephone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.6700507614213198,\n",
       " 'max_in_scope_acc': 0.70995670995671,\n",
       " 'max_out_scope_acc': 0.7852760736196319,\n",
       " 'best_out_scope_acc': 0.49079754601226994,\n",
       " 'best_acc': 0.5989847715736041,\n",
       " 'best_in_scope_acc': 0.6753246753246753,\n",
       " 'best_thresh': 0.87}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.95)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "severe-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.87, 'acc': 1.0, 'in_scope_acc': 1.0, 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], y_train, train_pred, train_pred_prob,\n",
    "                           default_class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-wallet",
   "metadata": {},
   "source": [
    "### bert-base-uncased finetuned on preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "swedish-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Text_Preprocessing(keep_eng=False, remove_nonalpha=False, lower_case=True,\n",
    "                         remove_punkt=False, remove_stop=False, remove_numerals=False,\n",
    "                         spell_check=False, contraction=True,\n",
    "                         contraction_var=CONTRACTIONS, stem=False,\n",
    "                         lem=False, filter_pos=False, pos_var=('N', 'J'),\n",
    "                         tokenize=False, template_removal=False,\n",
    "                         template_start_string='', regex_cleaning=False,\n",
    "                         remove_ignore_words=False, ignore_words=IGNORE_WORDS,\n",
    "                         custom_stoplist=[], word_size=2, word_size_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "weekly-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15ea34916d44c599407de4a5f0111d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24687f626b0b4e3dabdc586f118c3cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.4 ms, sys: 83.1 ms, total: 170 ms\n",
      "Wall time: 264 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['sent_pre'] = df_train['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_train['sent_pre'] = preprocess_obj.fit_transform(df_train['sent_pre'])\n",
    "df_train['sent_pre'] = df_train['sent_pre'].swifter.apply(lambda x:\n",
    "                                                          re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_test['sent_pre'] = df_test['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_test['sent_pre'] = preprocess_obj.fit_transform(df_test['sent_pre'])\n",
    "df_test['sent_pre'] = df_test['sent_pre'].swifter.apply(lambda x:\n",
    "                                                        re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_train.fillna(value={'sent_pre': ''}, inplace=True)\n",
    "df_test.fillna(value={'sent_pre': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "norman-motion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                         sentence label  label_trans  \\\n",
       " 0                    You guys provide EMI option?   EMI            8   \n",
       " 1  Do you offer Zero Percent EMI payment options?   EMI            8   \n",
       " 2                                         0% EMI.   EMI            8   \n",
       " 3                                             EMI   EMI            8   \n",
       " 4                           I want in installment   EMI            8   \n",
       " \n",
       "                                          sent_pre  \n",
       " 0                    you guys provide emi option   \n",
       " 1  do you offer zero percent emi payment options   \n",
       " 2                                            emi   \n",
       " 3                                             emi  \n",
       " 4                           i want in installment  ,\n",
       "                                    sentence              label  label_trans  \\\n",
       " 0                   There are only 2 models  NO_NODES_DETECTED           21   \n",
       " 1                                    Single  NO_NODES_DETECTED           21   \n",
       " 2  What's difference between ergo and ortho         COMPARISON            5   \n",
       " 3                              Return order    RETURN_EXCHANGE           17   \n",
       " 4               Hai not recieved my product  DELAY_IN_DELIVERY            6   \n",
       " \n",
       "                                      sent_pre  \n",
       " 0                       there are only models  \n",
       " 1                                      single  \n",
       " 2  what has difference between ergo and ortho  \n",
       " 3                                return order  \n",
       " 4                 hai not recieved my product  )"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(), df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "addressed-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 100\n",
    "FT_PRE_BERT_OUTPUT_DIR = os.path.join(INTER_DATA_DIR, \"ft_pre_bert_output\")\n",
    "FT_PRE_BERT_LOGS_DIR = os.path.join(INTER_DATA_DIR, \"ft_pre_bert_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "proved-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['sent_pre'].tolist()\n",
    "train_labels = df_train['label_trans'].tolist()\n",
    "test_texts = df_test['sent_pre'].tolist()\n",
    "test_labels = df_test['label_trans'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "massive-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 161 ms, sys: 125 ms, total: 285 ms\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "asian-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=list(set(train_labels)),\n",
    "                               y=train_labels).tolist()\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "alternative-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IntentDataset(train_encodings, weights, train_labels)\n",
    "val_dataset = IntentDataset(encodings=val_encodings, labels=train_labels)\n",
    "test_dataset = IntentDataset(encodings=test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "prerequisite-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 53:40, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.130712</td>\n",
       "      <td>0.789722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.352767</td>\n",
       "      <td>0.056885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>0.015832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.017160</td>\n",
       "      <td>0.009433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.006823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.004571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>0.004063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.003604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49min 52s, sys: 3min 49s, total: 53min 42s\n",
      "Wall time: 53min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1050, training_loss=0.24647830054873512)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FT_PRE_BERT_OUTPUT_DIR,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=FT_PRE_BERT_LOGS_DIR,\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=0.00004,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluate_during_training=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(set(train_labels)))\n",
    "\n",
    "trainer = MultiClassTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "single-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(FT_PRE_BERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "residential-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction...\n",
      "CPU times: user 83.8 ms, sys: 124 ms, total: 208 ms\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Prediction...\")\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained(FT_BERT_OUTPUT_DIR)\n",
    "test_trainer = Trainer(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "overall-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='91' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41/41 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 881 ms, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = IntentDataset(train_encodings)\n",
    "train_pred, train_pred_prob = get_predictions(test_trainer, train_dataset)\n",
    "test_pred, test_pred_prob = get_predictions(test_trainer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "environmental-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "CPU times: user 155 ms, sys: 15.1 ms, total: 170 ms\n",
      "Wall time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "default_class_idx = label2id[DEFAULT_CLASS]\n",
    "print(default_class_idx)\n",
    "y_test = df_test['label_trans'].values\n",
    "y_train = df_train['label_trans'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, test_pred, test_pred_prob,\n",
    "                                   default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "refined-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.6802030456852792,\n",
       " 'max_in_scope_acc': 0.7835497835497836,\n",
       " 'max_out_scope_acc': 0.8220858895705522,\n",
       " 'best_out_scope_acc': 0.50920245398773,\n",
       " 'best_acc': 0.6446700507614214,\n",
       " 'best_in_scope_acc': 0.7402597402597403,\n",
       " 'best_thresh': 0.8}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.943)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "rocky-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.8, 'acc': 1.0, 'in_scope_acc': 1.0, 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], y_train, train_pred, train_pred_prob,\n",
    "                           default_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "invisible-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# in scope examples in test:  231\n",
      "F1 Score: 0.7834\n"
     ]
    }
   ],
   "source": [
    "# f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_test_series = pd.Series(y_test)\n",
    "y_pred_series = pd.Series(test_pred)\n",
    "mask = y_test_series != 21\n",
    "print('# in scope examples in test: ', mask.sum())\n",
    "f_score = f1_score(y_true=y_test_series[mask].values, y_pred=y_pred_series[mask].values,\n",
    "                   average='weighted')\n",
    "print('F1 Score: %0.4f' % (f_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-release",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "sufficient-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394, 7)\n",
      "                                   sentence              label  label_trans  \\\n",
      "0                   There are only 2 models  NO_NODES_DETECTED           21   \n",
      "1                                    Single  NO_NODES_DETECTED           21   \n",
      "2  What's difference between ergo and ortho         COMPARISON            5   \n",
      "3                              Return order    RETURN_EXCHANGE           17   \n",
      "4               Hai not recieved my product  DELAY_IN_DELIVERY            6   \n",
      "\n",
      "                                     sent_pre  pred        pred_label  \\\n",
      "0                       there are only models    16  PRODUCT_VARIANTS   \n",
      "1                                      single    16  PRODUCT_VARIANTS   \n",
      "2  what has difference between ergo and ortho     5        COMPARISON   \n",
      "3                                return order    17   RETURN_EXCHANGE   \n",
      "4                 hai not recieved my product    17   RETURN_EXCHANGE   \n",
      "\n",
      "   pred_prob  \n",
      "0   0.874091  \n",
      "1   0.770442  \n",
      "2   0.995672  \n",
      "3   0.962098  \n",
      "4   0.836622  \n"
     ]
    }
   ],
   "source": [
    "id2label = {v: k for k, v in label2id.items()}\n",
    "errors_df = df_test.copy()\n",
    "errors_df['pred'] = test_pred\n",
    "errors_df['pred_label'] = errors_df['pred'].apply(lambda x: id2label[x])\n",
    "errors_df['pred_prob'] = test_pred_prob\n",
    "print(errors_df.shape)\n",
    "print(errors_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "anticipated-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             sentence              label  \\\n",
      "0                             There are only 2 models  NO_NODES_DETECTED   \n",
      "1                                              Single  NO_NODES_DETECTED   \n",
      "4                         Hai not recieved my product  DELAY_IN_DELIVERY   \n",
      "10                    please send them after lockdown  NO_NODES_DETECTED   \n",
      "11  I have not recieved anything with regard to that.  NO_NODES_DETECTED   \n",
      "15  Is there anybody? I want to purchase this SOF ...  NO_NODES_DETECTED   \n",
      "16  No but i wann the product to be delivered as i...  NO_NODES_DETECTED   \n",
      "17                        Is it available in Lucknow?  NO_NODES_DETECTED   \n",
      "23                     i am not able to order pillows  NO_NODES_DETECTED   \n",
      "24          How can i purchase this product in nepal?  NO_NODES_DETECTED   \n",
      "\n",
      "    label_trans                                           sent_pre  pred  \\\n",
      "0            21                              there are only models    16   \n",
      "1            21                                             single    16   \n",
      "4             6                        hai not recieved my product    17   \n",
      "10           21                    please send them after lockdown     0   \n",
      "11           21  i have not recieved anything with regard to that     17   \n",
      "15           21  is there anybody i want to purchase this sof e...     9   \n",
      "16           21  no but i wann the product to be delivered as i...     7   \n",
      "17           21                        is it available in lucknow      7   \n",
      "23           21                     i am not able to order pillows    15   \n",
      "24           21          how can i purchase this product in nepal      7   \n",
      "\n",
      "               pred_label  pred_prob  \n",
      "0        PRODUCT_VARIANTS   0.874091  \n",
      "1        PRODUCT_VARIANTS   0.770442  \n",
      "4         RETURN_EXCHANGE   0.836622  \n",
      "10  100_NIGHT_TRIAL_OFFER   0.913118  \n",
      "11        RETURN_EXCHANGE   0.887599  \n",
      "15          ERGO_FEATURES   0.994516  \n",
      "16           DISTRIBUTORS   0.590697  \n",
      "17           DISTRIBUTORS   0.979307  \n",
      "23                PILLOWS   0.995703  \n",
      "24           DISTRIBUTORS   0.857485  \n"
     ]
    }
   ],
   "source": [
    "mask = errors_df['label_trans'] != errors_df['pred']\n",
    "print(errors_df.loc[mask, :].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-obligation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
