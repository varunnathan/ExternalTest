{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wound-courtesy",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "1. SentenceTransformers on raw text\n",
    "2. SentenceTransformers on preprocessed text (contraction, lower casing, only alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "little-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, swifter, re, random\n",
    "from constants import *\n",
    "from utility import *\n",
    "from preprocess_utils import _remove_non_ascii_characters\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicate-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "DEFAULT_CLASS = 'NO_NODES_DETECTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 ms, sys: 1.79 ms, total: 5.91 ms\n",
      "Wall time: 5.99 ms\n",
      "CPU times: user 3.38 ms, sys: 3.49 ms, total: 6.87 ms\n",
      "Wall time: 10.6 ms\n",
      "(324, 2) \t (394, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "%time df_train = pd.read_csv(TRAIN_FN)\n",
    "%time df_test = pd.read_csv(TEST_FN)\n",
    "\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_train.shape, '\\t', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-spell",
   "metadata": {},
   "source": [
    "## Sentence Transformers on raw lower cased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "drawn-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentence'] = df_train['sentence'].str.lower()\n",
    "df_test['sentence'] = df_test['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guilty-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "authentic-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT_SOF_MATTRESS\n",
      "ERGO_FEATURES\n",
      "ORDER_STATUS\n",
      "COMPARISON\n",
      "COD\n",
      "OFFERS\n",
      "ORTHO_FEATURES\n",
      "LEAD_GEN\n",
      "PRODUCT_VARIANTS\n",
      "DELAY_IN_DELIVERY\n",
      "EMI\n",
      "CHECK_PINCODE\n",
      "MATTRESS_COST\n",
      "WARRANTY\n",
      "DISTRIBUTORS\n",
      "CANCEL_ORDER\n",
      "PILLOWS\n",
      "100_NIGHT_TRIAL_OFFER\n",
      "SIZE_CUSTOMIZATION\n",
      "RETURN_EXCHANGE\n",
      "WHAT_SIZE_TO_ORDER\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(df_train['label']))\n",
    "num_negative_examples = 5\n",
    "random.seed(100)\n",
    "label_examples = {}\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    mask = df_train['label'] == label\n",
    "    pos_lst = df_train.loc[mask, 'sentence'].tolist()\n",
    "    neg_lst = random.sample(df_train.loc[~mask, 'sentence'].tolist(), num_negative_examples)\n",
    "    pos_comb_lst = list(combinations(pos_lst, 2))\n",
    "    pos_comb_lst = [x + (0.95,) for x in pos_comb_lst]\n",
    "    pos_neg_comb_lst = list(product(pos_lst, neg_lst))\n",
    "    pos_neg_comb_lst = [x + (0.05,) for x in pos_neg_comb_lst]\n",
    "    label_examples[label] = pos_comb_lst + pos_neg_comb_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collectible-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT_SOF_MATTRESS 110\n",
      "ERGO_FEATURES 110\n",
      "ORDER_STATUS 290\n",
      "COMPARISON 110\n",
      "COD 126\n",
      "OFFERS 95\n",
      "ORTHO_FEATURES 221\n",
      "LEAD_GEN 315\n",
      "PRODUCT_VARIANTS 315\n",
      "DELAY_IN_DELIVERY 110\n",
      "EMI 425\n",
      "CHECK_PINCODE 95\n",
      "MATTRESS_COST 315\n",
      "WARRANTY 95\n",
      "DISTRIBUTORS 693\n",
      "CANCEL_ORDER 95\n",
      "PILLOWS 95\n",
      "100_NIGHT_TRIAL_OFFER 243\n",
      "SIZE_CUSTOMIZATION 81\n",
      "RETURN_EXCHANGE 161\n",
      "WHAT_SIZE_TO_ORDER 266\n"
     ]
    }
   ],
   "source": [
    "for label in label_examples:\n",
    "    print(label, len(label_examples[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alleged-sequence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how is sof different from other mattress brands', 'why sof mattress', 0.95),\n",
       " ('how is sof different from other mattress brands',\n",
       "  'about sof mattress',\n",
       "  0.95)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_examples['ABOUT_SOF_MATTRESS'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conditional-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "equivalent-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4366 [<sentence_transformers.readers.InputExample.InputExample object at 0x176a4b250>, <sentence_transformers.readers.InputExample.InputExample object at 0x176a4b1f0>]\n"
     ]
    }
   ],
   "source": [
    "train_examples = []\n",
    "for label, examples in label_examples.items():\n",
    "    for example in examples:\n",
    "        train_examples.append(InputExample(texts=list(example[:2]), label=example[2]))\n",
    "print(len(train_examples), train_examples[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incoming-olympus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.1.0, however, your version is 0.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187fa80319ae4bb6bc54167fee69e4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408ecb497eab48dbae8d0b77d56611b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 13s, sys: 2min 52s, total: 21min 6s\n",
      "Wall time: 21min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SentenceTransformer('stsb-roberta-base-v2')\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "expensive-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 346 ms, sys: 554 ms, total: 899 ms\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save\n",
    "FT_SENTBERT_OUTPUT_DIR = os.path.join(INTER_DATA_DIR, \"ft_sentbert_output\")\n",
    "model.save(FT_SENTBERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "spatial-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(FT_SENTBERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fuzzy-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train example:  product comparison \t test example:  what's difference between ergo and ortho \t cosine sim:  tensor([[0.9186]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "test_ex = df_test.loc[2, 'sentence']\n",
    "train_ex = df_train.loc[69, 'sentence']\n",
    "test_ex_emb = model.encode([test_ex])\n",
    "train_ex_emb = model.encode([train_ex])\n",
    "cosine_sim = cos_sim(test_ex_emb, train_ex_emb)\n",
    "print('train example: ', train_ex, '\\t', 'test example: ', test_ex, '\\t',\n",
    "      'cosine sim: ', cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-nothing",
   "metadata": {},
   "source": [
    "## Approach\n",
    "1. Embed all examples in train\n",
    "2. For each test example: embed -> find k closest train examples along with their scores -> apply threshold logic on scores and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "unnecessary-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings\n",
      "\n",
      "CPU times: user 5.68 s, sys: 46.3 ms, total: 5.73 s\n",
      "Wall time: 5.73 s\n",
      "test embeddings\n",
      "\n",
      "CPU times: user 13.8 s, sys: 256 ms, total: 14 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(model, docs, n_splits=8):\n",
    "    docs1 = np.array_split(docs, n_splits)\n",
    "\n",
    "    train_embeddings = []\n",
    "    for i, doc_lst in enumerate(docs1):\n",
    "        tmp_out = model.encode(doc_lst.tolist())\n",
    "        train_embeddings.append(tmp_out)\n",
    "    train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
    "    return train_embeddings\n",
    "\n",
    "\n",
    "print('train embeddings\\n')\n",
    "docs = df_train['sentence'].tolist()\n",
    "%time train_embeddings = get_embeddings(model, docs)\n",
    "\n",
    "print('test embeddings\\n')\n",
    "docs = df_test['sentence'].tolist()\n",
    "%time test_embeddings = get_embeddings(model, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adapted-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_examples(test_emb, train_embs, train_texts, train_labels, topk=3):\n",
    "    out = []\n",
    "    for i, train_emb in enumerate(train_embs):\n",
    "        score = cos_sim(test_emb, train_emb).item()\n",
    "        out.append((i, score, train_texts[i], train_labels[i]))\n",
    "    out = sorted(out, key=lambda k: k[1], reverse=True)\n",
    "    return out[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "enclosed-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['sentence'].tolist()\n",
    "train_labels = df_train['label'].tolist()\n",
    "test_texts = df_test['sentence'].tolist()\n",
    "test_labels = df_test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "recognized-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are only 2 models\n",
      "CPU times: user 28.7 ms, sys: 1.1 ms, total: 29.8 ms\n",
      "Wall time: 29 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(233, 0.8776043653488159, 'type of mattress', 'PRODUCT_VARIANTS'),\n",
       " (224, 0.8752570748329163, 'which product is best', 'PRODUCT_VARIANTS'),\n",
       " (68, 0.8676466941833496, 'compare the 2 mattresses', 'COMPARISON')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "print(test_texts[i])\n",
    "get_top_examples(test_embeddings[i], train_embeddings, train_texts, train_labels, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "laughing-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "(394, 6)\n",
      "                                  test_text         test_label  pred_idx  \\\n",
      "0                   there are only 2 models  NO_NODES_DETECTED       233   \n",
      "1                                    single  NO_NODES_DETECTED       159   \n",
      "2  what's difference between ergo and ortho         COMPARISON        66   \n",
      "3                              return order    RETURN_EXCHANGE       284   \n",
      "4               hai not recieved my product  DELAY_IN_DELIVERY       258   \n",
      "\n",
      "   pred_score                                 pred_text         pred_label  \n",
      "0    0.877604                          type of mattress   PRODUCT_VARIANTS  \n",
      "1    0.552013                         do you deliver to      CHECK_PINCODE  \n",
      "2    0.998691  difference between ergo & ortho mattress         COMPARISON  \n",
      "3    0.956194             help me with exchange process    RETURN_EXCHANGE  \n",
      "4    0.908211                       delivery is delayed  DELAY_IN_DELIVERY  \n",
      "CPU times: user 10.3 s, sys: 21.3 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_preds = []\n",
    "for j, test_emb in enumerate(test_embeddings):\n",
    "    if j % 50 == 0:\n",
    "        print(j)\n",
    "    test_pred = get_top_examples(test_emb, train_embeddings, train_texts, train_labels, 1)\n",
    "    d = {'test_text': test_texts[j], 'test_label': test_labels[j],\n",
    "         'pred_idx': test_pred[0][0], 'pred_score': test_pred[0][1],\n",
    "         'pred_text': test_pred[0][2], 'pred_label': test_pred[0][3]}\n",
    "    test_preds.append(d)\n",
    "    \n",
    "test_preds = pd.DataFrame(test_preds)\n",
    "print(test_preds.shape)\n",
    "print(test_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "existing-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "(324, 6)\n",
      "                                       train_text train_label  pred_idx  \\\n",
      "0                    you guys provide emi option?         EMI         0   \n",
      "1  do you offer zero percent emi payment options?         EMI         1   \n",
      "2                                         0% emi.         EMI         2   \n",
      "3                                             emi         EMI         3   \n",
      "4                           i want in installment         EMI         4   \n",
      "\n",
      "   pred_score                                       pred_text pred_label  \n",
      "0    1.000000                    you guys provide emi option?        EMI  \n",
      "1    1.000000  do you offer zero percent emi payment options?        EMI  \n",
      "2    0.999999                                         0% emi.        EMI  \n",
      "3    1.000000                                             emi        EMI  \n",
      "4    1.000000                           i want in installment        EMI  \n",
      "CPU times: user 8.46 s, sys: 18.1 ms, total: 8.47 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preds = []\n",
    "for j, train_emb in enumerate(train_embeddings):\n",
    "    if j % 50 == 0:\n",
    "        print(j)\n",
    "    train_pred = get_top_examples(train_emb, train_embeddings, train_texts, train_labels, 1)\n",
    "    d = {'train_text': train_texts[j], 'train_label': train_labels[j],\n",
    "         'pred_idx': train_pred[0][0], 'pred_score': train_pred[0][1],\n",
    "         'pred_text': train_pred[0][2], 'pred_label': train_pred[0][3]}\n",
    "    train_preds.append(d)\n",
    "    \n",
    "train_preds = pd.DataFrame(train_preds)\n",
    "print(train_preds.shape)\n",
    "print(train_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "technological-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (train_preds['train_label'] == train_preds['pred_label']).sum() == train_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bizarre-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_based_predictions(preds, probs, threshold, default_class):\n",
    "    preds_n = np.array([pred if probs[i] >= threshold else default_class\n",
    "                        for i, pred in enumerate(preds)])\n",
    "    return preds_n\n",
    "\n",
    "\n",
    "def get_accuracy(y_true, pred):\n",
    "    return np.mean(y_true == pred)\n",
    "\n",
    "\n",
    "def get_accuracy_per_threshold(threshold, y_true, preds, probs, default_class):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    preds = get_threshold_based_predictions(preds, probs, threshold, default_class)\n",
    "    acc = get_accuracy(y_true, preds)\n",
    "    pred_series = pd.Series(preds)\n",
    "    mask = y_true_series == default_class\n",
    "    in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "    out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "    d = {'threshold': threshold, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "         'out_scope_acc': out_scope_acc}\n",
    "    return d\n",
    "\n",
    "\n",
    "def find_optimal_threshold(thresholds, y_true, preds, probs, default_class):\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    \n",
    "    out = []\n",
    "    for thresh in thresholds:\n",
    "        preds = get_threshold_based_predictions(preds, probs, thresh, default_class)\n",
    "        acc = get_accuracy(y_true, preds)\n",
    "        pred_series = pd.Series(preds)\n",
    "        mask = y_true_series == default_class\n",
    "        in_scope_acc = get_accuracy(y_true_series[~mask].values, pred_series[~mask].values)\n",
    "        out_scope_acc = get_accuracy(y_true_series[mask].values, pred_series[mask].values)\n",
    "        d = {'threshold': thresh, 'acc': acc, 'in_scope_acc': in_scope_acc,\n",
    "             'out_scope_acc': out_scope_acc}\n",
    "        out.append(d)\n",
    "    out = pd.DataFrame(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def summarize_results(thresh_df, min_in_scope_acc_frac=0.95):\n",
    "    max_acc = thresh_df['acc'].max()\n",
    "    max_in_scope_acc = thresh_df['in_scope_acc'].max()\n",
    "    max_out_scope_acc = thresh_df['out_scope_acc'].max()\n",
    "    min_in_scope_acc = min_in_scope_acc_frac * max_in_scope_acc\n",
    "    mask = thresh_df['in_scope_acc'] >= min_in_scope_acc\n",
    "    best_out_scope_acc = thresh_df.loc[mask, 'out_scope_acc'].max()\n",
    "    mask1 = thresh_df['out_scope_acc'] == best_out_scope_acc\n",
    "    best_acc = thresh_df.loc[mask&mask1, 'acc'].values[0]\n",
    "    best_in_scope_acc = thresh_df.loc[mask&mask1, 'in_scope_acc'].values[0]\n",
    "    best_thresh = thresh_df.loc[mask&mask1, 'threshold'].values[0]\n",
    "    return {'max_acc': max_acc, 'max_in_scope_acc': max_in_scope_acc,\n",
    "            'max_out_scope_acc': max_out_scope_acc, 'best_out_scope_acc': best_out_scope_acc,\n",
    "            'best_acc': best_acc, 'best_in_scope_acc': best_in_scope_acc,\n",
    "            'best_thresh': best_thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "mobile-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CLASS = 'NO_NODES_DETECTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "surrounded-father",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 161 ms, sys: 5.95 ms, total: 167 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "y_test = df_test['label'].values\n",
    "y_train = df_train['label'].values\n",
    "test_pred = test_preds['pred_label'].values\n",
    "test_pred_prob = test_preds['pred_score'].values\n",
    "train_pred = train_preds['pred_label'].values\n",
    "train_pred_prob = train_preds['pred_score'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, test_pred, test_pred_prob,\n",
    "                                   DEFAULT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "equipped-bottle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.6802030456852792,\n",
       " 'max_in_scope_acc': 0.7402597402597403,\n",
       " 'max_out_scope_acc': 1.0,\n",
       " 'best_out_scope_acc': 0.49693251533742333,\n",
       " 'best_acc': 0.6142131979695431,\n",
       " 'best_in_scope_acc': 0.696969696969697,\n",
       " 'best_thresh': 0.82}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.94)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "generous-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.82, 'acc': 1.0, 'in_scope_acc': 1.0, 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], y_train, train_pred, train_pred_prob,\n",
    "                           DEFAULT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "stable-greene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# in scope examples in test:  231\n",
      "F1 Score: 0.7392\n"
     ]
    }
   ],
   "source": [
    "# f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_test_series = pd.Series(y_test)\n",
    "y_pred_series = pd.Series(test_pred)\n",
    "mask = y_test_series != DEFAULT_CLASS\n",
    "print('# in scope examples in test: ', mask.sum())\n",
    "f_score = f1_score(y_true=y_test_series[mask].values, y_pred=y_pred_series[mask].values,\n",
    "                   average='weighted')\n",
    "print('F1 Score: %0.4f' % (f_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-insulation",
   "metadata": {},
   "source": [
    "## Sentence Transformers on preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "equal-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Text_Preprocessing(keep_eng=False, remove_nonalpha=False, lower_case=True,\n",
    "                         remove_punkt=False, remove_stop=False, remove_numerals=False,\n",
    "                         spell_check=False, contraction=True,\n",
    "                         contraction_var=CONTRACTIONS, stem=False,\n",
    "                         lem=False, filter_pos=False, pos_var=('N', 'J'),\n",
    "                         tokenize=False, template_removal=False,\n",
    "                         template_start_string='', regex_cleaning=False,\n",
    "                         remove_ignore_words=False, ignore_words=IGNORE_WORDS,\n",
    "                         custom_stoplist=[], word_size=2, word_size_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "protected-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788fbdab50c4c4e96fa62c2404442b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction\n",
      "lower case\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a37be430734bbdaa0e4099e4c85f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.2 ms, sys: 11.5 ms, total: 85.7 ms\n",
      "Wall time: 89 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train['sent_pre'] = df_train['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_train['sent_pre'] = preprocess_obj.fit_transform(df_train['sent_pre'])\n",
    "df_train['sent_pre'] = df_train['sent_pre'].swifter.apply(lambda x:\n",
    "                                                          re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_test['sent_pre'] = df_test['sentence'].apply(lambda x: _remove_non_ascii_characters(x))\n",
    "df_test['sent_pre'] = preprocess_obj.fit_transform(df_test['sent_pre'])\n",
    "df_test['sent_pre'] = df_test['sent_pre'].swifter.apply(lambda x:\n",
    "                                                        re.sub(\"[^A-Za-z']+\", ' ', x))\n",
    "df_train.fillna(value={'sent_pre': ''}, inplace=True)\n",
    "df_test.fillna(value={'sent_pre': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "close-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT_SOF_MATTRESS\n",
      "ERGO_FEATURES\n",
      "ORDER_STATUS\n",
      "COMPARISON\n",
      "COD\n",
      "OFFERS\n",
      "ORTHO_FEATURES\n",
      "LEAD_GEN\n",
      "PRODUCT_VARIANTS\n",
      "DELAY_IN_DELIVERY\n",
      "EMI\n",
      "CHECK_PINCODE\n",
      "MATTRESS_COST\n",
      "WARRANTY\n",
      "DISTRIBUTORS\n",
      "CANCEL_ORDER\n",
      "PILLOWS\n",
      "100_NIGHT_TRIAL_OFFER\n",
      "SIZE_CUSTOMIZATION\n",
      "RETURN_EXCHANGE\n",
      "WHAT_SIZE_TO_ORDER\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(df_train['label']))\n",
    "num_negative_examples = 15\n",
    "random.seed(100)\n",
    "label_examples = {}\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    mask = df_train['label'] == label\n",
    "    pos_lst = df_train.loc[mask, 'sent_pre'].tolist()\n",
    "    neg_lst = random.sample(df_train.loc[~mask, 'sent_pre'].tolist(), num_negative_examples)\n",
    "    pos_comb_lst = list(combinations(pos_lst, 2))\n",
    "    pos_comb_lst = [x + (0.95,) for x in pos_comb_lst]\n",
    "    pos_neg_comb_lst = list(product(pos_lst, neg_lst))\n",
    "    pos_neg_comb_lst = [x + (0.05,) for x in pos_neg_comb_lst]\n",
    "    label_examples[label] = pos_comb_lst + pos_neg_comb_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "graphic-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT_SOF_MATTRESS 220\n",
      "ERGO_FEATURES 220\n",
      "ORDER_STATUS 490\n",
      "COMPARISON 220\n",
      "COD 246\n",
      "OFFERS 195\n",
      "ORTHO_FEATURES 391\n",
      "LEAD_GEN 525\n",
      "PRODUCT_VARIANTS 525\n",
      "DELAY_IN_DELIVERY 220\n",
      "EMI 675\n",
      "CHECK_PINCODE 195\n",
      "MATTRESS_COST 525\n",
      "WARRANTY 195\n",
      "DISTRIBUTORS 1023\n",
      "CANCEL_ORDER 195\n",
      "PILLOWS 195\n",
      "100_NIGHT_TRIAL_OFFER 423\n",
      "SIZE_CUSTOMIZATION 171\n",
      "RETURN_EXCHANGE 301\n",
      "WHAT_SIZE_TO_ORDER 456\n"
     ]
    }
   ],
   "source": [
    "for label in label_examples:\n",
    "    print(label, len(label_examples[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "concerned-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7606 [<sentence_transformers.readers.InputExample.InputExample object at 0x10f361fd0>, <sentence_transformers.readers.InputExample.InputExample object at 0x10f361f70>]\n",
      "CPU times: user 10.6 ms, sys: 1.09 ms, total: 11.7 ms\n",
      "Wall time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_examples = []\n",
    "for label, examples in label_examples.items():\n",
    "    for example in examples:\n",
    "        train_examples.append(InputExample(texts=list(example[:2]), label=example[2]))\n",
    "print(len(train_examples), train_examples[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "downtown-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_PRE_SENTBERT_OUTPUT_DIR = os.path.join(INTER_DATA_DIR, \"ft_pre_sentbert_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "super-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.1.0, however, your version is 0.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda12df23b284d13b4149a77960f5f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1379ecc4ab424ece8c44f1ea7bd579eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 43s, sys: 4min 9s, total: 30min 53s\n",
      "Wall time: 30min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SentenceTransformer('stsb-roberta-base-v2')\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100,\n",
    "          output_path=FT_PRE_SENTBERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "removable-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 480 ms, sys: 418 ms, total: 898 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save\n",
    "model.save(FT_PRE_SENTBERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "collective-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings\n",
      "\n",
      "CPU times: user 5.46 s, sys: 42.8 ms, total: 5.5 s\n",
      "Wall time: 5.5 s\n",
      "test embeddings\n",
      "\n",
      "CPU times: user 14.3 s, sys: 266 ms, total: 14.5 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "print('train embeddings\\n')\n",
    "docs = df_train['sent_pre'].tolist()\n",
    "%time train_embeddings = get_embeddings(model, docs)\n",
    "\n",
    "print('test embeddings\\n')\n",
    "docs = df_test['sent_pre'].tolist()\n",
    "%time test_embeddings = get_embeddings(model, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "editorial-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['sent_pre'].tolist()\n",
    "train_labels = df_train['label'].tolist()\n",
    "test_texts = df_test['sent_pre'].tolist()\n",
    "test_labels = df_test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "imperial-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "(394, 6)\n",
      "                                    test_text         test_label  pred_idx  \\\n",
      "0                       there are only models  NO_NODES_DETECTED       218   \n",
      "1                                      single  NO_NODES_DETECTED       119   \n",
      "2  what has difference between ergo and ortho         COMPARISON        66   \n",
      "3                                return order    RETURN_EXCHANGE       273   \n",
      "4                 hai not recieved my product  DELAY_IN_DELIVERY       255   \n",
      "\n",
      "   pred_score                                      pred_text  \\\n",
      "0    0.857281                               product variants   \n",
      "1    0.489505                                                  \n",
      "2    0.957130         difference between ergo ortho mattress   \n",
      "3    0.809019                                 present status   \n",
      "4    0.964084  it has been days my product have not received   \n",
      "\n",
      "           pred_label  \n",
      "0    PRODUCT_VARIANTS  \n",
      "1  WHAT_SIZE_TO_ORDER  \n",
      "2          COMPARISON  \n",
      "3        ORDER_STATUS  \n",
      "4   DELAY_IN_DELIVERY  \n",
      "CPU times: user 9.33 s, sys: 9.69 ms, total: 9.34 s\n",
      "Wall time: 9.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_preds = []\n",
    "for j, test_emb in enumerate(test_embeddings):\n",
    "    if j % 50 == 0:\n",
    "        print(j)\n",
    "    test_pred = get_top_examples(test_emb, train_embeddings, train_texts, train_labels, 1)\n",
    "    d = {'test_text': test_texts[j], 'test_label': test_labels[j],\n",
    "         'pred_idx': test_pred[0][0], 'pred_score': test_pred[0][1],\n",
    "         'pred_text': test_pred[0][2], 'pred_label': test_pred[0][3]}\n",
    "    test_preds.append(d)\n",
    "    \n",
    "test_preds = pd.DataFrame(test_preds)\n",
    "print(test_preds.shape)\n",
    "print(test_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "growing-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "(324, 6)\n",
      "                                       train_text train_label  pred_idx  \\\n",
      "0                    you guys provide emi option          EMI         0   \n",
      "1  do you offer zero percent emi payment options          EMI         1   \n",
      "2                                            emi          EMI         2   \n",
      "3                                             emi         EMI         3   \n",
      "4                           i want in installment         EMI         4   \n",
      "\n",
      "   pred_score                                       pred_text pred_label  \n",
      "0    1.000000                    you guys provide emi option         EMI  \n",
      "1    1.000000  do you offer zero percent emi payment options         EMI  \n",
      "2    1.000000                                            emi         EMI  \n",
      "3    1.000000                                             emi        EMI  \n",
      "4    1.000001                           i want in installment        EMI  \n",
      "CPU times: user 7.66 s, sys: 9.28 ms, total: 7.67 s\n",
      "Wall time: 7.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preds = []\n",
    "for j, train_emb in enumerate(train_embeddings):\n",
    "    if j % 50 == 0:\n",
    "        print(j)\n",
    "    train_pred = get_top_examples(train_emb, train_embeddings, train_texts, train_labels, 1)\n",
    "    d = {'train_text': train_texts[j], 'train_label': train_labels[j],\n",
    "         'pred_idx': train_pred[0][0], 'pred_score': train_pred[0][1],\n",
    "         'pred_text': train_pred[0][2], 'pred_label': train_pred[0][3]}\n",
    "    train_preds.append(d)\n",
    "    \n",
    "train_preds = pd.DataFrame(train_preds)\n",
    "print(train_preds.shape)\n",
    "print(train_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "humanitarian-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (train_preds['train_label'] == train_preds['pred_label']).sum() == train_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "pleased-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 149 ms, sys: 6.1 ms, total: 155 ms\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thresholds = [x/100. for x in range(100)]\n",
    "y_test = df_test['label'].values\n",
    "y_train = df_train['label'].values\n",
    "test_pred = test_preds['pred_label'].values\n",
    "test_pred_prob = test_preds['pred_score'].values\n",
    "train_pred = train_preds['pred_label'].values\n",
    "train_pred_prob = train_preds['pred_score'].values\n",
    "thresh_df = find_optimal_threshold(thresholds, y_test, test_pred, test_pred_prob,\n",
    "                                   DEFAULT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "suspended-budapest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_acc': 0.6395939086294417,\n",
       " 'max_in_scope_acc': 0.7186147186147186,\n",
       " 'max_out_scope_acc': 0.901840490797546,\n",
       " 'best_out_scope_acc': 0.5153374233128835,\n",
       " 'best_acc': 0.6065989847715736,\n",
       " 'best_in_scope_acc': 0.670995670995671,\n",
       " 'best_thresh': 0.72}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_d = summarize_results(thresh_df, min_in_scope_acc_frac=0.93)\n",
    "best_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "covered-sheriff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/varunnathan/.virtualenvs/py38/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.72, 'acc': 1.0, 'in_scope_acc': 1.0, 'out_scope_acc': nan}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train results\n",
    "get_accuracy_per_threshold(best_d['best_thresh'], y_train, train_pred, train_pred_prob,\n",
    "                           DEFAULT_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "single-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# in scope examples in test:  231\n",
      "F1 Score: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# f1_score\n",
    "y_test_series = pd.Series(y_test)\n",
    "y_pred_series = pd.Series(test_pred)\n",
    "mask = y_test_series != DEFAULT_CLASS\n",
    "print('# in scope examples in test: ', mask.sum())\n",
    "f_score = f1_score(y_true=y_test_series[mask].values, y_pred=y_pred_series[mask].values,\n",
    "                   average='weighted')\n",
    "print('F1 Score: %0.4f' % (f_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-reporter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
