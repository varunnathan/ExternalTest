{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/ExternalTest/MAD/src/\")\n",
    "from constants import *\n",
    "from metadata_utils import _find_files\n",
    "from baseline_feats_utils import feat_type_feats_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx_dct = {'GE20': USER2IDX_SEGGE20_FN,\n",
    "                'LE3': USER2IDX_SEGLE3_FN,\n",
    "                '4-19': USER2IDX_SEG4TO19_FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, sample, model_type, file_name=None,\n",
    "                 interim_data_dir=INTERIM_DATA_DIR, user_col=USER_COL,\n",
    "                 item_col=ITEM_COL, ontology_col=ONTOLOGY_COL,\n",
    "                 brand_col=BRAND_COL, price_col=PRICE_COL, dv_col=DV_COL,\n",
    "                 date_col=DATE_COL, end_token='.gz', chunksize=10,\n",
    "                 segment='LE3', user2idx_dct=user2idx_dct):\n",
    "\n",
    "        data_dir = interim_data_dir\n",
    "        \n",
    "        if file_name is None:\n",
    "            files = _find_files(data_dir, end_token)\n",
    "            if sample == 'train':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if not x.startswith('0005')]\n",
    "            elif sample == 'test':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if x.startswith('0005')]\n",
    "        else:\n",
    "            self.files = [os.path.join(data_dir, file_name)]\n",
    "        print(self.files)\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.segment = segment\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.ontology_col = ontology_col\n",
    "        self.brand_col = brand_col\n",
    "        self.price_col = price_col\n",
    "        self.date_col = date_col\n",
    "        self.dv_col = dv_col\n",
    "        self.feat_type_feats_dct = feat_type_feats_dct\n",
    "        self.chunksize = chunksize\n",
    "        user_feats = ['{}_{}'.format(self.user_col, x) for x in\n",
    "                      self.feat_type_feats_dct['user']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        user_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.user_col))\n",
    "        item_feats = ['{}_{}'.format(self.item_col, x) for x in\n",
    "                      self.feat_type_feats_dct['item']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        item_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.item_col))\n",
    "        self.numeric_feats = [self.price_col] + user_feats + item_feats\n",
    "        if self.segment == 'GE20':\n",
    "            self.cat_feats = [self.user_col, self.item_col,\n",
    "                              self.ontology_col, self.brand_col]\n",
    "        else:\n",
    "            self.cat_feats = [self.item_col, self.ontology_col,\n",
    "                              self.brand_col]\n",
    "        if self.segment == 'GE20':\n",
    "            self.user2idx = json.load(open(user2idx_dct.get(self.segment)))\n",
    "        else:\n",
    "            self.user2idx = None\n",
    "        \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        df = pd.read_csv(fn, compression='gzip', sep='|', iterator=True,\n",
    "                         chunksize=self.chunksize)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_dv_for_classification(self, dv_lst):\n",
    "        \n",
    "        if self.model_type == 'classification':\n",
    "            return [int(x-1) for x in dv_lst]\n",
    "        else:\n",
    "            return [int(x) for x in dv_lst]\n",
    "        \n",
    "    \n",
    "    def _segment_filter(self, num_interactions_lst, feat_type, feats_lst):\n",
    "        \n",
    "        if self.segment != 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x < 20]\n",
    "        elif self.segment == 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x >= 20]\n",
    "        \n",
    "        if idxs:\n",
    "            new_feats_lst = [feats_lst[i] for i in idxs]\n",
    "            if (self.segment == 'GE20') and (feat_type == 'cat'):\n",
    "                new_feats_lst = []\n",
    "                for i in idxs:\n",
    "                    out = feats_lst[i]\n",
    "                    out[0] = self.user2idx[str(out[0])]\n",
    "                    new_feats_lst.append(out)\n",
    "            return new_feats_lst\n",
    "\n",
    "    \n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        for row in data:\n",
    "            num_interactions = row['uuid_num_interactions'].values.tolist()\n",
    "            \n",
    "            x1 = row[self.cat_feats].values.tolist()\n",
    "            x2 = row[self.numeric_feats].values.tolist()\n",
    "            y = self.get_dv_for_classification(\n",
    "                    row[self.dv_col].tolist())\n",
    "            x1 = self._segment_filter(num_interactions, 'cat', x1)\n",
    "            if x1:\n",
    "                x2 = self._segment_filter(num_interactions, 'numeric', x2)\n",
    "                y = self._segment_filter(num_interactions, 'dv', y)\n",
    "                yield (x1, x2, y)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRecommendationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for product recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for\n",
    "                                         categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont, self.n_classes = n_emb, n_cont, n_classes\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 300)\n",
    "        self.lin2 = nn.Linear(300, 100)\n",
    "        self.lin3 = nn.Linear(100, self.n_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as torch_optim\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def choose_embedding_size(cat_cols, cat_num_values, min_emb_dim=100):\n",
    "    \"\"\"\n",
    "    cat_cols: list of categorical columns\n",
    "    cat_num_values: list of number of unique values for each categorical column\n",
    "    \"\"\"\n",
    "\n",
    "    embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "    embedding_sizes = [(n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "                       for _, n_categories in embedded_cols.items()]\n",
    "    return embedding_sizes\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "\n",
    "def construct_tensor(a):\n",
    "\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def train_model(model, optim, train_dl, train_size, chunksize, batch_size,\n",
    "                device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in train_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            output = model(x1, x2)\n",
    "            loss = loss_fn(output, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl, test_size, chunksize, batch_size,\n",
    "             device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    sum_auc_macro = 0\n",
    "    sum_auc_micro = 0\n",
    "    num_aucs = 0\n",
    "    with tqdm(total=test_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in valid_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            pred_prob = F.softmax(out, dim=1)\n",
    "            y_onehot = F.one_hot(y)\n",
    "            correct += (pred == y).float().sum().item()\n",
    "            pred_prob = pred_prob.cpu().detach().numpy()\n",
    "            y_onehot = y_onehot.cpu().detach().numpy()\n",
    "            try:\n",
    "                sum_auc_macro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='macro')\n",
    "                sum_auc_micro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='micro')\n",
    "                num_aucs += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            pbar.update(1)\n",
    "    print(\"valid loss %.3f, accuracy %.3f, macro auc %.3f and micro auc %.3f\" % (\n",
    "        sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs))\n",
    "    return sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs\n",
    "\n",
    "\n",
    "def train_loop(model, train_dl, valid_dl, epochs, train_size,\n",
    "               test_size, chunksize, batch_size, device, lr=0.01,\n",
    "               wd=0.0, loss_fn=F.cross_entropy):\n",
    "\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        stats = {'epoch': i+1}\n",
    "        train_loss = train_model(model, optim, train_dl, train_size,\n",
    "                                 chunksize, batch_size, device,\n",
    "                                 loss_fn)\n",
    "        print(\"training loss: \", train_loss)\n",
    "        stats['train_loss'] = train_loss\n",
    "        loss, acc, auc_macro, auc_micro = val_loss(\n",
    "            model, valid_dl, test_size, chunksize, batch_size, device, loss_fn)\n",
    "        print('time taken: %0.2f' % (time.time() - start))\n",
    "        stats['test_loss'] = loss\n",
    "        stats['test_acc'] = acc\n",
    "        stats['test_auc_macro'] = auc_macro\n",
    "        stats['test_auc_micro'] = auc_micro\n",
    "        losses.append(stats)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "SEGMENT = 'GE20'\n",
    "N_USERS = 1444170\n",
    "N_ITEMS = 1175648\n",
    "N_ONTOLOGIES = 801\n",
    "N_BRANDS = 1686\n",
    "BATCH_SIZE = 20\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 173044425\n",
    "TEST_SIZE = 34608886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding size\n",
    "\n",
    "if SEGMENT != 'GE20':\n",
    "    cat_cols = [ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "else:\n",
    "    cat_cols = [USER_COL, ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_USERS, N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "\n",
    "embedding_sizes = choose_embedding_size(cat_cols, cat_num_values, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1444170, 150), (1175648, 150), (801, 150), (1686, 150)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ExternalTest_Data/MAD/interim/0000_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_00.gz']\n",
      "['/data/ExternalTest_Data/MAD/interim/0005_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_04.gz']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_name=None, model_type='classification',\n",
    "    sample='train', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(\n",
    "    file_name=None, model_type='classification',\n",
    "    sample='test', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numeric vars:  18\n"
     ]
    }
   ],
   "source": [
    "n_cont = len(train_loader.dataset.numeric_feats)\n",
    "print('number of numeric vars: ', n_cont)\n",
    "\n",
    "net = ProductRecommendationModel(embedding_sizes, n_cont, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1444170, 150)\n",
       "    (1): Embedding(1175648, 150)\n",
       "    (2): Embedding(801, 150)\n",
       "    (3): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=618, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1444170, 150)\n",
       "    (1): Embedding(1175648, 150)\n",
       "    (2): Embedding(801, 150)\n",
       "    (3): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=618, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/86522 [00:00<4:48:30,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2407/86522 [07:44<4:31:09,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3095/86522 [09:53<4:15:18,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3781/86522 [12:04<4:18:53,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6120/86522 [19:35<4:15:11,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8510/86522 [27:05<4:08:11,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 10917/86522 [34:46<3:56:15,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 13293/86522 [42:10<3:46:34,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 15633/86522 [49:38<3:46:05,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 17973/86522 [57:09<3:36:03,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 20343/86522 [1:04:30<3:23:02,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 22713/86522 [1:11:51<3:14:57,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 25061/86522 [1:19:13<3:05:36,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 27449/86522 [1:26:41<3:05:04,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 29797/86522 [1:34:02<2:50:34,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 32185/86522 [1:41:31<2:48:35,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 34575/86522 [1:48:58<2:59:54,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 36915/86522 [1:56:26<2:34:12,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 39286/86522 [2:03:48<2:24:34,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 39970/86522 [2:05:57<2:25:17,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 42362/86522 [2:13:24<2:16:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 44701/86522 [2:20:53<2:11:30,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 47078/86522 [2:28:16<1:59:42,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 49467/86522 [2:35:42<2:04:22,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 50150/86522 [2:37:49<1:55:48,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 50834/86522 [2:39:57<1:51:16,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 53224/86522 [2:47:23<1:44:05,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 55600/86522 [2:54:47<1:42:41,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 57992/86522 [3:02:14<1:28:29,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 60339/86522 [3:09:35<1:19:39,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 62729/86522 [3:17:04<1:14:58,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 65134/86522 [3:24:43<1:08:18,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 67481/86522 [3:32:10<58:13,  5.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 69829/86522 [3:39:37<50:57,  5.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 72235/86522 [3:47:23<46:01,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 74606/86522 [3:54:45<36:33,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 76982/86522 [4:02:12<29:30,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 79353/86522 [4:09:35<22:01,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 81743/86522 [4:17:06<15:41,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 84119/86522 [4:24:31<07:26,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86524it [4:32:14,  5.30it/s]                           \n",
      "  0%|          | 1/17304 [00:00<38:25,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0001807517974001042\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 2379/17304 [05:30<33:49,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4706/17304 [10:50<27:43,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 5391/17304 [12:23<26:24,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 7795/17304 [18:05<22:08,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 10128/17304 [23:34<16:33,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 12491/17304 [28:55<10:45,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 14874/17304 [34:17<05:36,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 17247/17304 [39:37<00:07,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.937, macro auc 0.639 and micro auc 0.973\n",
      "time taken: 18712.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model=net, train_dl=train_loader,\n",
    "                    valid_dl=test_loader, epochs=1,\n",
    "                    train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                    chunksize=CHUNKSIZE, batch_size=BATCH_SIZE,\n",
    "                    device=device, lr=0.02, wd=0.00001,\n",
    "                    loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.0001807517974001042,\n",
       "  'test_loss': 0.00020090498921754253,\n",
       "  'test_acc': 0.9368723338407677,\n",
       "  'test_auc_macro': 0.6392410360275745,\n",
       "  'test_auc_micro': 0.9732561114418942}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/py36/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ProductRecommendationModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model's state dict\n",
    "model_fn = os.path.join(MODEL_DIR, 'Class_model_SegGE20_E1.pt')\n",
    "torch.save(net.state_dict, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and checkpoint model\n",
    "model_ckpt_fn = os.path.join(MODEL_DIR, 'Class_model_SegGE20_E1_ckpt.pt')\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': get_optimizer(\n",
    "                net, lr = 0.02, wd = 0.00001).state_dict(),\n",
    "            'loss': losses[-1]['test_loss'],\n",
    "            'acc': losses[-1]['test_acc'],\n",
    "            'auc_macro': losses[-1]['test_auc_macro'],\n",
    "            'auc_micro': losses[-1]['test_auc_micro']\n",
    "            }, model_ckpt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "preds = []\n",
    "pred_probs = []\n",
    "actuals = []\n",
    "actuals_onehot = []\n",
    "with torch.no_grad():\n",
    "    for x1, x2, y in test_loader:\n",
    "        x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                     construct_tensor_y(y))\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)\n",
    "        out = net(x1, x2)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        pred_prob = F.softmax(out, dim=1)\n",
    "        y_onehot = F.one_hot(y)\n",
    "        preds.append(pred)\n",
    "        pred_probs.append(pred_prob)\n",
    "        actuals.append(y)\n",
    "        actuals_onehot.append(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17305,\n",
       " 17305,\n",
       " 17305,\n",
       " 17305,\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [0, 1, 0],\n",
       "         [1, 0, 0]], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(preds), len(pred_probs), len(actuals), len(actuals_onehot),\n",
    " actuals_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = [item for sublist in preds for\n",
    "               item in sublist.cpu().detach().tolist()]\n",
    "final_pred_probs = [item for sublist in pred_probs for\n",
    "                    item in sublist.cpu().detach().tolist()]\n",
    "final_actuals = [item for sublist in actuals for\n",
    "                 item in sublist.cpu().detach().tolist()]\n",
    "final_actuals_onehot = F.one_hot(tensor(final_actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23251612,\n",
       " 23251612,\n",
       " 23251612,\n",
       " 23251612,\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]]),\n",
       " [[0.9744861721992493, 0.022715192288160324, 0.002798564499244094],\n",
       "  [0.9864744544029236, 0.01223214715719223, 0.001293312176130712],\n",
       "  [0.9910107851028442, 0.008373735472559929, 0.0006154446164146066],\n",
       "  [0.9869650602340698, 0.011990916915237904, 0.0010439311154186726],\n",
       "  [0.9823639392852783, 0.016432181000709534, 0.0012038853019475937]],\n",
       " [0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(final_preds), len(final_pred_probs), len(final_actuals),\n",
    " len(final_actuals_onehot), final_actuals_onehot[:5],\n",
    " final_pred_probs[:5], final_actuals[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_probs = np.array(final_pred_probs)\n",
    "final_actuals_onehot = np.array(final_actuals_onehot)\n",
    "final_actuals = np.array(final_actuals)\n",
    "final_preds = np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23251612, 3)\n",
      "(23251612, 3)\n",
      "[0.97448617 0.02271519 0.00279856]\n",
      "[[1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(final_pred_probs.shape)\n",
    "print(final_actuals_onehot.shape)\n",
    "print(final_pred_probs[0])\n",
    "print(final_actuals_onehot[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.937\n",
      "Test Macro AUC: 0.632\n",
      "Test Micro AUC: 0.974\n",
      "Test RMSE: 0.277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "acc = (final_actuals == final_preds).mean()\n",
    "auc_macro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='macro')\n",
    "auc_micro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='micro')\n",
    "rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                  y_pred=final_preds))\n",
    "\n",
    "print('Test Accuracy: %0.3f' % (acc))\n",
    "print('Test Macro AUC: %0.3f' % (auc_macro))\n",
    "print('Test Micro AUC: %0.3f' % (auc_micro))\n",
    "print('Test RMSE: %0.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23251612, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>actual_onehot</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9744861721992493, 0.022715192288160324, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9864744544029236, 0.01223214715719223, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9910107851028442, 0.008373735472559929, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9869650602340698, 0.011990916915237904, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9823639392852783, 0.016432181000709534, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual actual_onehot  pred_class  \\\n",
       "0       0     [1, 0, 0]           0   \n",
       "1       0     [1, 0, 0]           0   \n",
       "2       0     [1, 0, 0]           0   \n",
       "3       0     [1, 0, 0]           0   \n",
       "4       0     [1, 0, 0]           0   \n",
       "\n",
       "                                           pred_prob  \n",
       "0  [0.9744861721992493, 0.022715192288160324, 0.0...  \n",
       "1  [0.9864744544029236, 0.01223214715719223, 0.00...  \n",
       "2  [0.9910107851028442, 0.008373735472559929, 0.0...  \n",
       "3  [0.9869650602340698, 0.011990916915237904, 0.0...  \n",
       "4  [0.9823639392852783, 0.016432181000709534, 0.0...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(\n",
    "    {'actual': final_actuals,\n",
    "     'actual_onehot': final_actuals_onehot.tolist(),\n",
    "     'pred_class': final_preds,\n",
    "     'pred_prob': final_pred_probs.tolist()})\n",
    "\n",
    "print(prediction_df.shape)\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fn = os.path.join(PREDICTION_DIR, 'prediction_SegGE20_E1.csv.gz')\n",
    "prediction_df.to_csv(pred_fn, compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
