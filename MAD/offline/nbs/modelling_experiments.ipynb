{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. NN Classification\n",
    "2. NN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/ExternalTest/MAD/src/\")\n",
    "from constants import *\n",
    "from metadata_utils import _find_files\n",
    "from baseline_feats_utils import feat_type_feats_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables based on exploratory analysis\n",
    "USER2IDX_SEGGE20_FN = os.path.join(METADATA_DIR, 'user2idx_segGE20.json')\n",
    "USER2IDX_SEGLE3_FN = os.path.join(METADATA_DIR, 'user2idx_segLE3.json')\n",
    "USER2IDX_SEG4TO19_FN = os.path.join(METADATA_DIR, 'user2idx_seg4-19.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx_dct = {'GE20': USER2IDX_SEGGE20_FN,\n",
    "                'LE3': USER2IDX_SEGLE3_FN,\n",
    "                '4-19': USER2IDX_SEG4TO19_FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': ['num_interactions',\n",
       "  'mean_price_interactions',\n",
       "  'earliest_interaction_date',\n",
       "  'min_num_interactions_per_pdt',\n",
       "  'max_num_interactions_per_pdt',\n",
       "  'mean_num_interactions_per_pdt',\n",
       "  'min_num_interactions_per_ont',\n",
       "  'max_num_interactions_per_ont',\n",
       "  'mean_num_interactions_per_ont',\n",
       "  'min_num_interactions_per_brand',\n",
       "  'max_num_interactions_per_brand',\n",
       "  'mean_num_interactions_per_brand'],\n",
       " 'item': ['num_interactions',\n",
       "  'earliest_interaction_date',\n",
       "  'min_num_interactions_per_user',\n",
       "  'max_num_interactions_per_user',\n",
       "  'mean_num_interactions_per_user']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_type_feats_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - NN Classification with baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, sample, model_type, file_name=None,\n",
    "                 interim_data_dir=INTERIM_DATA_DIR, user_col=USER_COL,\n",
    "                 item_col=ITEM_COL, ontology_col=ONTOLOGY_COL,\n",
    "                 brand_col=BRAND_COL, price_col=PRICE_COL, dv_col=DV_COL,\n",
    "                 date_col=DATE_COL, end_token='.gz', chunksize=10,\n",
    "                 segment='LE3', user2idx_dct=user2idx_dct):\n",
    "\n",
    "        data_dir = interim_data_dir\n",
    "        \n",
    "        if file_name is None:\n",
    "            files = _find_files(data_dir, end_token)\n",
    "            if sample == 'train':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if not x.startswith('0005')]\n",
    "            elif sample == 'test':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if x.startswith('0005')]\n",
    "        else:\n",
    "            self.files = [os.path.join(data_dir, file_name)]\n",
    "        print(self.files)\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.segment = segment\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.ontology_col = ontology_col\n",
    "        self.brand_col = brand_col\n",
    "        self.price_col = price_col\n",
    "        self.date_col = date_col\n",
    "        self.dv_col = dv_col\n",
    "        self.feat_type_feats_dct = feat_type_feats_dct\n",
    "        self.chunksize = chunksize\n",
    "        user_feats = ['{}_{}'.format(self.user_col, x) for x in\n",
    "                      self.feat_type_feats_dct['user']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        user_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.user_col))\n",
    "        item_feats = ['{}_{}'.format(self.item_col, x) for x in\n",
    "                      self.feat_type_feats_dct['item']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        item_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.item_col))\n",
    "        self.numeric_feats = [self.price_col] + user_feats + item_feats\n",
    "        if self.segment == 'GE20':\n",
    "            self.cat_feats = [self.user_col, self.item_col,\n",
    "                              self.ontology_col, self.brand_col]\n",
    "        else:\n",
    "            self.cat_feats = [self.item_col, self.ontology_col,\n",
    "                              self.brand_col]\n",
    "        if self.segment == 'GE20':\n",
    "            self.user2idx = json.load(open(user2idx_dct.get(self.segment)))\n",
    "        else:\n",
    "            self.user2idx = None\n",
    "        \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        df = pd.read_csv(fn, compression='gzip', sep='|', iterator=True,\n",
    "                         chunksize=self.chunksize)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_dv_for_classification(self, dv_lst):\n",
    "        \n",
    "        if self.model_type == 'classification':\n",
    "            return [int(x-1) for x in dv_lst]\n",
    "        else:\n",
    "            return [int(x) for x in dv_lst]\n",
    "        \n",
    "    \n",
    "    def _segment_filter(self, num_interactions_lst, feat_type, feats_lst):\n",
    "        \n",
    "        if self.segment != 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x < 20]\n",
    "        elif self.segment == 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x >= 20]\n",
    "        \n",
    "        if idxs:\n",
    "            new_feats_lst = [feats_lst[i] for i in idxs]\n",
    "            if (self.segment == 'GE20') and (feat_type == 'cat'):\n",
    "                new_feats_lst = []\n",
    "                for i in idxs:\n",
    "                    out = feats_lst[i]\n",
    "                    out[0] = self.user2idx[str(out[0])]\n",
    "                    new_feats_lst.append(out)\n",
    "            return new_feats_lst\n",
    "\n",
    "    \n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        for row in data:\n",
    "            num_interactions = row['uuid_num_interactions'].values.tolist()\n",
    "            \n",
    "            x1 = row[self.cat_feats].values.tolist()\n",
    "            x2 = row[self.numeric_feats].values.tolist()\n",
    "            y = self.get_dv_for_classification(\n",
    "                    row[self.dv_col].tolist())\n",
    "            x1 = self._segment_filter(num_interactions, 'cat', x1)\n",
    "            if x1:\n",
    "                x2 = self._segment_filter(num_interactions, 'numeric', x2)\n",
    "                y = self._segment_filter(num_interactions, 'dv', y)\n",
    "                yield (x1, x2, y)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRecommendationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for product recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for\n",
    "                                         categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont, self.n_classes = n_emb, n_cont, n_classes\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 300)\n",
    "        self.lin2 = nn.Linear(300, 100)\n",
    "        self.lin3 = nn.Linear(100, self.n_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as torch_optim\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def choose_embedding_size(cat_cols, cat_num_values, min_emb_dim=100):\n",
    "    \"\"\"\n",
    "    cat_cols: list of categorical columns\n",
    "    cat_num_values: list of number of unique values for each categorical column\n",
    "    \"\"\"\n",
    "\n",
    "    embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "    embedding_sizes = [(n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "                       for _, n_categories in embedded_cols.items()]\n",
    "    return embedding_sizes\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "\n",
    "def construct_tensor(a):\n",
    "\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def train_model(model, optim, train_dl, train_size, chunksize, batch_size,\n",
    "                device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in train_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            output = model(x1, x2)\n",
    "            loss = loss_fn(output, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl, test_size, chunksize, batch_size,\n",
    "             device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    sum_auc_macro = 0\n",
    "    sum_auc_micro = 0\n",
    "    num_aucs = 0\n",
    "    with tqdm(total=test_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in valid_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            pred_prob = F.softmax(out, dim=1)\n",
    "            y_onehot = F.one_hot(y)\n",
    "            correct += (pred == y).float().sum().item()\n",
    "            pred_prob = pred_prob.cpu().detach().numpy()\n",
    "            y_onehot = y_onehot.cpu().detach().numpy()\n",
    "            try:\n",
    "                sum_auc_macro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='macro')\n",
    "                sum_auc_micro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='micro')\n",
    "                num_aucs += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            pbar.update(1)\n",
    "    print(\"valid loss %.3f, accuracy %.3f, macro auc %.3f and micro auc %.3f\" % (\n",
    "        sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs))\n",
    "    return sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs\n",
    "\n",
    "\n",
    "def train_loop(model, train_dl, valid_dl, epochs, train_size,\n",
    "               test_size, chunksize, batch_size, device, lr=0.01,\n",
    "               wd=0.0, loss_fn=F.cross_entropy):\n",
    "\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        stats = {'epoch': i+1}\n",
    "        train_loss = train_model(model, optim, train_dl, train_size,\n",
    "                                 chunksize, batch_size, device,\n",
    "                                 loss_fn)\n",
    "        print(\"training loss: \", train_loss)\n",
    "        stats['train_loss'] = train_loss\n",
    "        loss, acc, auc_macro, auc_micro = val_loss(\n",
    "            model, valid_dl, test_size, chunksize, batch_size, device, loss_fn)\n",
    "        print('time taken: %0.2f' % (time.time() - start))\n",
    "        stats['test_loss'] = loss\n",
    "        stats['test_acc'] = acc\n",
    "        stats['test_auc_macro'] = auc_macro\n",
    "        stats['test_auc_micro'] = auc_micro\n",
    "        losses.append(stats)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment - < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "TRAIN_FILE_NAME = '0000_part_00.gz'\n",
    "TEST_FILE_NAME = '0005_part_07.gz'\n",
    "SEGMENT = 'LE3'\n",
    "N_USERS = 4881444\n",
    "N_ITEMS = 1175648\n",
    "N_ONTOLOGIES = 801\n",
    "N_BRANDS = 1686\n",
    "BATCH_SIZE = 50\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 4812995 # corresponds to FILE_NAME\n",
    "TEST_SIZE = 1371989    # corresponds to FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding size\n",
    "\n",
    "if SEGMENT != 'GE20':\n",
    "    cat_cols = [ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "else:\n",
    "    cat_cols = [USER_COL, ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_USERS, N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "\n",
    "embedding_sizes = choose_embedding_size(cat_cols, cat_num_values, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1175648, 150), (801, 150), (1686, 150)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ExternalTest_Data/MAD/interim/0000_part_00.gz']\n",
      "['/data/ExternalTest_Data/MAD/interim/0005_part_07.gz']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_name=TRAIN_FILE_NAME, model_type='classification',\n",
    "    sample='train', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(\n",
    "    file_name=TEST_FILE_NAME, model_type='classification',\n",
    "    sample='test', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numeric vars:  18\n"
     ]
    }
   ],
   "source": [
    "n_cont = len(train_loader.dataset.numeric_feats)\n",
    "print('number of numeric vars: ', n_cont)\n",
    "\n",
    "net = ProductRecommendationModel(embedding_sizes, n_cont, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1175648, 150)\n",
       "    (1): Embedding(801, 150)\n",
       "    (2): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=468, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1175648, 150)\n",
       "    (1): Embedding(801, 150)\n",
       "    (2): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=468, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "tensor([[ 720408,     727,     346],\n",
      "        [ 674750,     783,    1147],\n",
      "        [ 886279,     512,    1592],\n",
      "        [ 922935,     217,     757],\n",
      "        [ 247773,     104,     808],\n",
      "        [ 631158,     241,     965],\n",
      "        [1004812,     617,    1133],\n",
      "        [ 208059,     104,     547],\n",
      "        [1158573,     165,     101],\n",
      "        [1134965,     659,    1246],\n",
      "        [ 173913,     657,    1327],\n",
      "        [ 133463,     783,    1437],\n",
      "        [ 134684,     512,    1144],\n",
      "        [ 902684,     165,     862],\n",
      "        [ 681135,     104,     235],\n",
      "        [1158573,     165,     101],\n",
      "        [ 489617,      87,    1229],\n",
      "        [ 574000,     512,    1246],\n",
      "        [1030658,     757,    1211],\n",
      "        [ 611497,     696,     444],\n",
      "        [ 884114,     423,    1647],\n",
      "        [ 273820,     427,     832],\n",
      "        [  71791,     737,     124],\n",
      "        [  63868,     512,    1246],\n",
      "        [ 493093,     217,    1327],\n",
      "        [ 492524,     512,    1508],\n",
      "        [ 494617,     512,    1246],\n",
      "        [ 140999,     104,      46],\n",
      "        [ 359872,     757,     883],\n",
      "        [ 303725,     104,     279],\n",
      "        [1032800,     423,     454],\n",
      "        [ 631570,     419,    1685],\n",
      "        [1057419,     423,     454],\n",
      "        [  40515,     695,     906],\n",
      "        [ 390662,     792,    1578],\n",
      "        [ 701014,     512,    1144],\n",
      "        [  96134,     104,     235],\n",
      "        [ 364950,     423,     454],\n",
      "        [1051307,     695,     906],\n",
      "        [ 383756,     104,     808],\n",
      "        [ 768121,     699,    1058],\n",
      "        [  75608,     581,    1058],\n",
      "        [ 116622,       0,     187],\n",
      "        [ 150787,     783,    1437],\n",
      "        [ 106220,     427,     906],\n",
      "        [   8259,      24,    1645],\n",
      "        [1013148,     431,    1203],\n",
      "        [ 143566,     208,    1104],\n",
      "        [ 428406,     104,    1674],\n",
      "        [ 685356,     783,    1512],\n",
      "        [ 955234,     591,    1327],\n",
      "        [ 938911,     243,     187],\n",
      "        [ 116740,     423,     454],\n",
      "        [ 774200,     699,    1058],\n",
      "        [ 625939,     512,    1246],\n",
      "        [  77430,     708,     226],\n",
      "        [ 732633,     277,     497],\n",
      "        [1175312,     512,    1144],\n",
      "        [ 172006,     277,     497],\n",
      "        [1014041,     512,    1246],\n",
      "        [ 325907,     783,    1203],\n",
      "        [ 380856,     619,    1144],\n",
      "        [1078863,     696,       3],\n",
      "        [ 704026,     277,     497],\n",
      "        [1117132,     696,     832],\n",
      "        [ 545800,     416,     131],\n",
      "        [  84064,     217,     757],\n",
      "        [ 634758,     591,    1327],\n",
      "        [ 103788,      26,     859],\n",
      "        [ 894686,     104,     808],\n",
      "        [  14709,     431,     616],\n",
      "        [ 221952,     269,    1168],\n",
      "        [ 164176,     598,     883],\n",
      "        [ 850831,     269,     679],\n",
      "        [ 315631,     512,    1147],\n",
      "        [ 230735,     512,    1246],\n",
      "        [ 799719,     104,     275],\n",
      "        [ 177627,     783,      27],\n",
      "        [ 563983,     269,     757],\n",
      "        [ 658550,     336,     381],\n",
      "        [ 437134,     695,    1437],\n",
      "        [1084407,      87,     150],\n",
      "        [ 894392,     512,    1437],\n",
      "        [ 612327,     783,    1203],\n",
      "        [1099029,      21,     969],\n",
      "        [  46927,     512,    1246],\n",
      "        [ 969954,     136,     187],\n",
      "        [ 504268,     790,      81],\n",
      "        [ 539879,     512,    1246],\n",
      "        [ 509508,     512,    1144],\n",
      "        [ 170433,     423,     454],\n",
      "        [ 317907,     359,     142],\n",
      "        [ 915858,     623,    1506],\n",
      "        [ 299161,     512,    1246],\n",
      "        [ 524568,     602,    1578],\n",
      "        [1022890,     423,     454],\n",
      "        [ 713100,     512,    1147],\n",
      "        [ 714154,     607,    1640],\n",
      "        [ 441055,     512,    1246],\n",
      "        [ 468460,     783,    1203],\n",
      "        [ 247443,      17,     130],\n",
      "        [  96162,     512,    1480],\n",
      "        [ 612327,     783,    1203],\n",
      "        [  99675,      19,    1053],\n",
      "        [ 932675,     783,    1203],\n",
      "        [ 844451,     695,    1293],\n",
      "        [ 590846,     696,     444],\n",
      "        [ 550117,     783,    1203],\n",
      "        [  23237,     394,     897],\n",
      "        [ 681135,     104,     235],\n",
      "        [1134159,     431,     161],\n",
      "        [ 598316,      21,     338],\n",
      "        [ 354210,     423,     454],\n",
      "        [ 192147,     363,    1659],\n",
      "        [ 302811,     757,      92],\n",
      "        [ 463663,     448,     434],\n",
      "        [ 615312,     757,     883],\n",
      "        [ 805528,     431,     616],\n",
      "        [1052454,     437,    1058],\n",
      "        [ 893251,     423,     454],\n",
      "        [ 680854,     696,    1578],\n",
      "        [ 660690,     104,     275],\n",
      "        [ 361166,     704,      92],\n",
      "        [1057419,     423,     454],\n",
      "        [ 884586,     200,     829],\n",
      "        [ 967774,     512,    1592],\n",
      "        [  16524,     783,    1203],\n",
      "        [ 194683,     217,     933],\n",
      "        [ 843735,     336,     381],\n",
      "        [ 684974,     217,    1210],\n",
      "        [ 676944,     431,     161],\n",
      "        [ 597568,     431,    1480],\n",
      "        [ 782112,     659,    1144],\n",
      "        [ 808753,     104,     808],\n",
      "        [ 737223,     210,     838],\n",
      "        [1002128,     542,     187],\n",
      "        [  12690,     423,     454],\n",
      "        [ 402282,     512,    1647],\n",
      "        [ 389684,     269,     688],\n",
      "        [ 352715,     270,       8],\n",
      "        [ 527217,     199,    1437],\n",
      "        [ 427614,     764,     497],\n",
      "        [ 831701,     623,      27],\n",
      "        [ 965456,     104,     633],\n",
      "        [ 371732,     431,    1319],\n",
      "        [ 352928,     659,    1246],\n",
      "        [ 321007,     421,    1437],\n",
      "        [ 252047,     512,    1246],\n",
      "        [ 450813,     210,     863],\n",
      "        [ 117913,     484,    1058],\n",
      "        [ 490907,      12,     969],\n",
      "        [ 341890,     431,      27],\n",
      "        [ 911783,     783,    1203],\n",
      "        [ 226111,     512,    1246],\n",
      "        [ 428390,     783,    1506],\n",
      "        [1010291,     512,    1246],\n",
      "        [  17404,     764,     637],\n",
      "        [1075840,     165,     101],\n",
      "        [ 365498,      21,     338],\n",
      "        [1007683,     512,    1246],\n",
      "        [ 230083,     635,    1058],\n",
      "        [ 611554,     336,    1457],\n",
      "        [ 255365,     217,    1327],\n",
      "        [ 145440,     261,    1645],\n",
      "        [1141604,     512,    1144],\n",
      "        [ 935901,     296,     525],\n",
      "        [ 605368,     269,     757],\n",
      "        [ 133583,     356,     946],\n",
      "        [  67686,     512,    1246],\n",
      "        [ 483500,     277,     497],\n",
      "        [ 167985,     720,    1661],\n",
      "        [1030796,     329,    1327],\n",
      "        [ 417500,     602,    1370],\n",
      "        [ 264841,     783,    1506],\n",
      "        [ 827943,     462,     121],\n",
      "        [1094201,     423,     454],\n",
      "        [1057419,     423,     454],\n",
      "        [ 298351,     364,     294],\n",
      "        [1114658,     788,     895],\n",
      "        [ 875189,     462,     121],\n",
      "        [ 418202,     512,    1144],\n",
      "        [ 141049,     623,      27],\n",
      "        [ 283913,     269,    1327],\n",
      "        [ 435151,     462,     708],\n",
      "        [ 831741,     512,    1246],\n",
      "        [1057419,     423,     454],\n",
      "        [ 932779,     431,     616],\n",
      "        [ 250375,     431,    1203],\n",
      "        [ 513744,      87,     352],\n",
      "        [ 765218,     104,     279],\n",
      "        [ 550192,     747,     302],\n",
      "        [1088966,     431,     616],\n",
      "        [ 967774,     512,    1592],\n",
      "        [ 830639,     676,      92],\n",
      "        [ 349488,     512,    1246],\n",
      "        [1136807,     512,    1246],\n",
      "        [ 103966,     180,     114],\n",
      "        [ 808615,     512,    1144],\n",
      "        [ 346260,     783,      27],\n",
      "        [ 164478,     512,    1246],\n",
      "        [ 343272,     431,    1168],\n",
      "        [ 713100,     512,    1147],\n",
      "        [ 383756,     104,     808],\n",
      "        [ 996826,     431,     616],\n",
      "        [ 486104,     269,    1327],\n",
      "        [ 500343,     512,    1246],\n",
      "        [ 826374,     423,     454],\n",
      "        [ 383428,     512,    1246],\n",
      "        [  59013,     423,     454],\n",
      "        [1165506,     696,    1327],\n",
      "        [ 524128,     431,    1506],\n",
      "        [ 387134,     109,     114],\n",
      "        [ 848006,     269,    1327],\n",
      "        [ 590532,     757,     275],\n",
      "        [ 805528,     431,     616],\n",
      "        [ 189127,     437,     132],\n",
      "        [ 981152,     659,    1144],\n",
      "        [ 653780,     709,    1058],\n",
      "        [1007683,     512,    1246],\n",
      "        [ 619142,     282,     883],\n",
      "        [ 605070,     659,    1246],\n",
      "        [ 528569,     708,     142],\n",
      "        [ 471194,     423,     454],\n",
      "        [  55581,     512,    1246],\n",
      "        [ 186464,     104,    1674],\n",
      "        [ 524172,     512,    1246],\n",
      "        [  63868,     512,    1246],\n",
      "        [ 443298,     615,    1135],\n",
      "        [ 305448,     328,     226],\n",
      "        [ 186464,     104,    1674],\n",
      "        [1057419,     423,     454],\n",
      "        [ 390930,     538,     895],\n",
      "        [ 545760,     659,    1144],\n",
      "        [ 117680,     512,    1437],\n",
      "        [ 900845,     423,     454],\n",
      "        [ 194539,     614,     616],\n",
      "        [ 136926,     623,      27],\n",
      "        [ 100441,     423,     454],\n",
      "        [ 952750,     104,     547],\n",
      "        [ 944270,     512,    1508],\n",
      "        [1151082,     104,     235],\n",
      "        [ 721117,     104,     294],\n",
      "        [ 118649,     512,     682],\n",
      "        [ 191980,     512,    1144],\n",
      "        [ 170046,     104,     808],\n",
      "        [ 107687,      46,      92],\n",
      "        [ 116566,     512,    1144],\n",
      "        [ 200970,     699,    1058],\n",
      "        [ 287592,     512,    1013],\n",
      "        [  84064,     217,     757]]) \t torch.Size([250, 3])\n",
      "\n",
      "\n",
      "tensor([[4.4119e+04, 1.0000e+00, 4.4119e+04,  ..., 8.0000e+00, 1.0639e+00,\n",
      "         0.0000e+00],\n",
      "        [1.2990e+03, 2.0000e+00, 1.0490e+03,  ..., 7.0000e+00, 1.0166e+00,\n",
      "         1.4421e-02],\n",
      "        [7.9900e+02, 7.0000e+00, 1.1552e+03,  ..., 9.0000e+00, 1.0309e+00,\n",
      "         3.8264e-02],\n",
      "        ...,\n",
      "        [6.9900e+02, 1.4000e+01, 7.9957e+03,  ..., 1.7000e+01, 1.0454e+00,\n",
      "         3.0831e-01],\n",
      "        [1.8990e+03, 1.2000e+01, 1.4267e+03,  ..., 6.0000e+00, 1.6201e+00,\n",
      "         0.0000e+00],\n",
      "        [3.2990e+03, 1.4000e+01, 3.9634e+03,  ..., 1.4000e+01, 1.0755e+00,\n",
      "         2.2454e-01]])\n",
      "\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \t torch.Size([250])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 815583,     423,     454],\n",
      "        [ 232899,     619,    1144],\n",
      "        [1142639,     336,     381],\n",
      "        ...,\n",
      "        [ 532674,     799,    1394],\n",
      "        [ 832950,     423,     454],\n",
      "        [   5648,     708,     226]]) \t torch.Size([650, 3])\n",
      "\n",
      "\n",
      "tensor([[2.7990e+03, 9.0000e+00, 8.1775e+02,  ..., 9.0000e+00, 1.0533e+00,\n",
      "         5.0611e-01],\n",
      "        [5.9900e+02, 5.0000e+00, 1.7678e+03,  ..., 6.0000e+00, 1.0651e+00,\n",
      "         3.6043e-01],\n",
      "        [9.4500e+02, 2.0000e+00, 9.4500e+02,  ..., 4.7000e+01, 1.1414e+00,\n",
      "         2.1497e-01],\n",
      "        ...,\n",
      "        [6.5000e+03, 3.0000e+00, 6.5000e+03,  ..., 7.0000e+00, 1.0535e+00,\n",
      "         5.2946e-01],\n",
      "        [2.7990e+03, 4.0000e+00, 2.5240e+03,  ..., 1.0000e+01, 1.0386e+00,\n",
      "         3.9329e-02],\n",
      "        [4.4900e+02, 1.6000e+01, 4.9469e+03,  ..., 9.0000e+00, 1.1287e+00,\n",
      "         2.0370e-03]])\n",
      "\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]) \t torch.Size([650])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for x1, x2, y in islice(train_loader, 2):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    print(x1, '\\t', x1.shape)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y, '\\t', y.shape)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "tensor([[530312,    676,    301],\n",
      "        [344594,    382,    660],\n",
      "        [442471,    104,    238],\n",
      "        ...,\n",
      "        [778171,    678,    660],\n",
      "        [ 42138,    431,     27],\n",
      "        [338068,    364,   1182]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[1.8750e+04, 1.0000e+00, 1.7500e+04,  ..., 5.0000e+00, 1.0357e+00,\n",
      "         6.6850e+01],\n",
      "        [3.4590e+04, 3.0000e+00, 4.4765e+04,  ..., 2.4000e+01, 1.1236e+00,\n",
      "         6.7049e+01],\n",
      "        [2.1999e+04, 1.1000e+01, 1.5918e+04,  ..., 2.1000e+01, 1.1140e+00,\n",
      "         6.5578e+01],\n",
      "        ...,\n",
      "        [1.3990e+04, 1.0000e+00, 9.1000e+03,  ..., 7.0000e+00, 1.0286e+00,\n",
      "         4.3703e+01],\n",
      "        [9.9900e+02, 1.8000e+01, 5.2206e+03,  ..., 7.0000e+00, 1.0964e+00,\n",
      "         4.3920e+01],\n",
      "        [2.5990e+04, 8.0000e+00, 3.3349e+04,  ..., 1.2000e+01, 1.0694e+00,\n",
      "         6.8808e+01]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shape of y:  torch.Size([400])\n",
      "shape of x1:  torch.Size([400, 3])\n",
      "shape of x2:  torch.Size([400, 18])\n",
      "model output:  tensor([[-0.2344,  0.7968, -0.1468],\n",
      "        [ 0.6857, -1.0346, -1.0452],\n",
      "        [ 0.1944,  0.9897,  0.0841],\n",
      "        ...,\n",
      "        [ 0.0185, -0.1181,  0.0278],\n",
      "        [ 0.5026,  0.5835, -0.5143],\n",
      "        [-0.8998, -0.5925, -0.7020]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Loss:  1.1635432243347168\n",
      "tensor([[ 491982,     538,    1034],\n",
      "        [ 656002,      87,    1229],\n",
      "        [1028718,     305,    1497],\n",
      "        ...,\n",
      "        [ 121839,     336,     381],\n",
      "        [  33591,     792,     829],\n",
      "        [ 965821,     737,     753]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[3.9063e+04, 7.0000e+00, 2.4479e+03,  ..., 3.6000e+01, 1.1114e+00,\n",
      "         2.7253e+01],\n",
      "        [9.9000e+02, 3.0000e+00, 9.9000e+02,  ..., 2.3000e+01, 1.1737e+00,\n",
      "         5.7791e+01],\n",
      "        [4.3990e+04, 1.2000e+01, 5.5855e+04,  ..., 2.6000e+01, 1.0757e+00,\n",
      "         5.3432e+01],\n",
      "        ...,\n",
      "        [2.3950e+03, 4.0000e+00, 8.7365e+03,  ..., 2.2000e+01, 1.1532e+00,\n",
      "         1.3838e+02],\n",
      "        [6.9990e+03, 2.0000e+00, 6.9990e+03,  ..., 3.0000e+00, 1.0159e+00,\n",
      "         7.6894e+01],\n",
      "        [1.2691e+05, 4.0000e+00, 8.1724e+04,  ..., 1.0000e+00, 1.0000e+00,\n",
      "         1.0024e+01]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shape of y:  torch.Size([600])\n",
      "shape of x1:  torch.Size([600, 3])\n",
      "shape of x2:  torch.Size([600, 18])\n",
      "model output:  tensor([[-0.2831, -0.5327,  0.2960],\n",
      "        [ 0.2043, -0.2890,  0.6797],\n",
      "        [-0.3937,  0.2937, -0.0513],\n",
      "        ...,\n",
      "        [-0.3360,  0.5148,  0.3527],\n",
      "        [ 0.9598, -1.7786, -1.0700],\n",
      "        [ 0.3887,  0.5778,  0.2710]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Loss:  1.1498897075653076\n"
     ]
    }
   ],
   "source": [
    "for x1, x2, y in islice(test_loader, 2):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    y = y.to(device)\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print('\\n\\n\\n')\n",
    "    print('shape of y: ', y.size())\n",
    "    print('shape of x1: ', x1.size())\n",
    "    print('shape of x2: ', x2.size())\n",
    "    out = net(x1, x2)\n",
    "    print('model output: ', out)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    print('Loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/962 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963it [04:40,  3.43it/s]                         \n",
      "  0%|          | 0/274 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0005126195600893918\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 145/274 [01:14<01:06,  1.95it/s]\n",
      "  0%|          | 0/962 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.926, macro auc 0.525 and micro auc 0.967\n",
      "time taken: 354.64\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963it [04:41,  3.42it/s]                         \n",
      "  0%|          | 0/274 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0004887084268028866\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 145/274 [01:16<01:08,  1.89it/s]\n",
      "  0%|          | 0/962 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.933, macro auc 0.557 and micro auc 0.969\n",
      "time taken: 713.12\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963it [04:48,  3.33it/s]                         \n",
      "  0%|          | 0/274 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0004896451841191066\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 145/274 [01:16<01:08,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.928, macro auc 0.559 and micro auc 0.966\n",
      "time taken: 1078.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model=net, train_dl=train_loader,\n",
    "                    valid_dl=test_loader, epochs=3,\n",
    "                    train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                    chunksize=CHUNKSIZE, batch_size=BATCH_SIZE,\n",
    "                    device=device, lr=0.02, wd=0.00001,\n",
    "                    loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.0005126195600893918,\n",
       "  'test_loss': 0.00044533589014774744,\n",
       "  'test_acc': 0.9257399659073299,\n",
       "  'test_auc_macro': 0.5251515309534442,\n",
       "  'test_auc_micro': 0.9667040375041938},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 0.0004887084268028866,\n",
       "  'test_loss': 0.0004100935259715875,\n",
       "  'test_acc': 0.933364326669766,\n",
       "  'test_auc_macro': 0.5570571219986813,\n",
       "  'test_auc_micro': 0.9694310671958396},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 0.0004896451841191066,\n",
       "  'test_loss': 0.0004525444943295622,\n",
       "  'test_acc': 0.9276873805465158,\n",
       "  'test_auc_macro': 0.5594984226243153,\n",
       "  'test_auc_micro': 0.9662497053403207}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment >= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "TRAIN_FILE_NAME = '0000_part_00.gz'\n",
    "TEST_FILE_NAME = '0005_part_07.gz'\n",
    "SEGMENT = '>=20'\n",
    "N_USERS = 1444170\n",
    "N_ITEMS = 1175648\n",
    "N_ONTOLOGIES = 801\n",
    "N_BRANDS = 1686\n",
    "BATCH_SIZE = 20\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 4812995 # corresponds to FILE_NAME\n",
    "TEST_SIZE = 1371989    # corresponds to FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding size\n",
    "\n",
    "if SEGMENT == '<20':\n",
    "    cat_cols = [ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "else:\n",
    "    cat_cols = [USER_COL, ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_USERS, N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "\n",
    "embedding_sizes = choose_embedding_size(cat_cols, cat_num_values, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1444170, 150), (1175648, 150), (801, 150), (1686, 150)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ExternalTest_Data/MAD/interim/0000_part_00.gz']\n",
      "['/data/ExternalTest_Data/MAD/interim/0005_part_07.gz']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_name=TRAIN_FILE_NAME, model_type='classification',\n",
    "    sample='train', chunksize=CHUNKSIZE, segment='>=20')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(\n",
    "    file_name=TEST_FILE_NAME, model_type='classification',\n",
    "    sample='test', chunksize=CHUNKSIZE, segment='>=20')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numeric vars:  18\n"
     ]
    }
   ],
   "source": [
    "n_cont = len(train_loader.dataset.numeric_feats)\n",
    "print('number of numeric vars: ', n_cont)\n",
    "\n",
    "net = ProductRecommendationModel(embedding_sizes, n_cont, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1444170, 150)\n",
       "    (1): Embedding(1175648, 150)\n",
       "    (2): Embedding(801, 150)\n",
       "    (3): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=618, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1444170, 150)\n",
       "    (1): Embedding(1175648, 150)\n",
       "    (2): Embedding(801, 150)\n",
       "    (3): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=618, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uuid', 'sourceprodid', 'ontology', 'brand']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "tensor([[      0, 1084662,     431,    1356],\n",
      "        [     84,  383428,     512,    1246],\n",
      "        [    166, 1082098,     217,    1327],\n",
      "        ...,\n",
      "        [   1236,  867060,     695,     906],\n",
      "        [   1300,  459801,     696,    1437],\n",
      "        [   1356,  424042,     739,     708]]) \t torch.Size([1540, 4])\n",
      "\n",
      "\n",
      "tensor([[8.9900e+02, 9.2000e+01, 1.7840e+03,  ..., 6.0000e+00, 1.0883e+00,\n",
      "         4.0625e-03],\n",
      "        [9.9900e+02, 5.7700e+02, 1.7922e+03,  ..., 1.5000e+01, 1.0766e+00,\n",
      "         4.3403e-03],\n",
      "        [4.1990e+03, 3.2000e+01, 5.2756e+03,  ..., 7.0000e+00, 1.0177e+00,\n",
      "         5.9375e-03],\n",
      "        ...,\n",
      "        [1.2990e+03, 3.9460e+03, 1.6930e+03,  ..., 6.0000e+00, 1.0583e+00,\n",
      "         4.2176e-02],\n",
      "        [7.9900e+02, 5.9000e+01, 3.6607e+03,  ..., 1.1000e+01, 1.0694e+00,\n",
      "         2.2638e-01],\n",
      "        [6.9900e+02, 5.9500e+02, 3.4337e+03,  ..., 4.0000e+00, 1.0638e+00,\n",
      "         0.0000e+00]])\n",
      "\n",
      "\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0]) \t torch.Size([1540])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[    543, 1018293,     423,     454],\n",
      "        [   1215,  612659,     266,    1058],\n",
      "        [    601,  619840,     695,     906],\n",
      "        ...,\n",
      "        [   2489,  486831,     423,     454],\n",
      "        [   2550,  497452,     512,    1480],\n",
      "        [     90,  170433,     423,     454]]) \t torch.Size([1480, 4])\n",
      "\n",
      "\n",
      "tensor([[2.5990e+03, 2.4160e+03, 4.1791e+03,  ..., 1.3000e+01, 1.0903e+00,\n",
      "         7.2465e-02],\n",
      "        [1.4990e+03, 1.3110e+03, 2.0784e+03,  ..., 1.0000e+01, 1.1776e+00,\n",
      "         8.7986e-02],\n",
      "        [1.3990e+03, 1.9800e+02, 9.9903e+02,  ..., 4.0000e+00, 1.0158e+00,\n",
      "         6.1678e-02],\n",
      "        ...,\n",
      "        [1.5990e+03, 2.8000e+01, 1.0794e+03,  ..., 1.1000e+01, 1.1048e+00,\n",
      "         2.9097e-02],\n",
      "        [1.2990e+03, 7.9800e+02, 1.0234e+03,  ..., 7.0000e+00, 1.0349e+00,\n",
      "         1.5208e-02],\n",
      "        [2.5990e+03, 2.1000e+01, 2.1884e+03,  ..., 8.0000e+00, 1.0893e+00,\n",
      "         4.9340e-02]])\n",
      "\n",
      "\n",
      "tensor([0, 0, 0,  ..., 0, 0, 1]) \t torch.Size([1480])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for x1, x2, y in islice(train_loader, 2):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    print(x1, '\\t', x1.shape)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y, '\\t', y.shape)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "tensor([[ 659123,  911340,     431,    1480],\n",
      "        [  71264,  329759,     222,    1437],\n",
      "        [ 445621,  898016,     591,    1327],\n",
      "        ...,\n",
      "        [1015707,  677610,     277,    1197],\n",
      "        [ 484325,   18975,     542,     557],\n",
      "        [ 841773,  838605,     512,    1246]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[1.0990e+03, 7.7600e+02, 2.6948e+03,  ..., 4.0000e+00, 1.0271e+00,\n",
      "         3.8233e+01],\n",
      "        [2.9900e+02, 4.7700e+02, 1.0494e+03,  ..., 5.0000e+00, 1.0723e+00,\n",
      "         1.2464e+02],\n",
      "        [5.2950e+03, 2.0500e+02, 7.3025e+03,  ..., 1.3000e+01, 1.0593e+00,\n",
      "         5.2573e+01],\n",
      "        ...,\n",
      "        [1.6990e+03, 2.8000e+01, 2.6732e+03,  ..., 6.0000e+00, 1.0906e+00,\n",
      "         5.2098e+01],\n",
      "        [1.2999e+04, 3.7000e+02, 2.6119e+04,  ..., 1.4000e+01, 1.0940e+00,\n",
      "         5.5663e+01],\n",
      "        [1.4990e+03, 2.4900e+02, 4.9271e+03,  ..., 5.0000e+00, 1.0452e+00,\n",
      "         2.6743e+01]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shape of y:  torch.Size([1440])\n",
      "shape of x1:  torch.Size([1440, 4])\n",
      "model output:  tensor([[ 0.1462, -1.6484, -0.8881],\n",
      "        [ 0.0439, -0.1479, -0.5584],\n",
      "        [ 0.3448,  0.1075, -0.1119],\n",
      "        ...,\n",
      "        [ 0.0652, -0.3998,  0.0720],\n",
      "        [ 0.5696, -0.8624,  0.9397],\n",
      "        [-0.6936,  1.4123,  0.6471]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Loss:  1.1422829627990723\n",
      "tensor([[ 379248,  570078,     269,    1327],\n",
      "        [ 229577,  409955,     277,     952],\n",
      "        [ 611637,  794468,     512,    1085],\n",
      "        ...,\n",
      "        [ 326044,  465281,     423,    1147],\n",
      "        [1134670,  949492,     269,    1249],\n",
      "        [1078457,  353310,     104,    1331]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[2.2990e+03, 5.8700e+02, 2.7041e+03,  ..., 5.0000e+00, 1.0522e+00,\n",
      "         5.0527e+01],\n",
      "        [3.9900e+03, 3.8100e+02, 1.4819e+04,  ..., 2.0000e+00, 1.1487e+00,\n",
      "         6.6179e+01],\n",
      "        [1.4990e+03, 1.5300e+02, 1.4855e+03,  ..., 4.0000e+00, 1.1598e+00,\n",
      "         3.7775e+01],\n",
      "        ...,\n",
      "        [1.9990e+03, 4.8100e+02, 1.0282e+04,  ..., 5.0000e+00, 1.0633e+00,\n",
      "         5.2418e+01],\n",
      "        [9.9900e+02, 6.0000e+01, 2.0889e+03,  ..., 5.0000e+00, 1.0737e+00,\n",
      "         5.4925e+01],\n",
      "        [1.0590e+04, 5.8000e+01, 1.0547e+04,  ..., 3.6000e+01, 1.1544e+00,\n",
      "         5.2688e+01]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shape of y:  torch.Size([1500])\n",
      "shape of x1:  torch.Size([1500, 4])\n",
      "model output:  tensor([[ 0.0332, -0.3544, -0.8793],\n",
      "        [ 0.3258, -0.6322, -0.1111],\n",
      "        [-1.4475, -0.2385, -0.2083],\n",
      "        ...,\n",
      "        [-1.3950,  0.5484, -0.3377],\n",
      "        [ 0.1889, -0.7457,  0.4995],\n",
      "        [ 0.4918, -0.1429, -0.5491]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "Loss:  1.1463645696640015\n"
     ]
    }
   ],
   "source": [
    "for x1, x2, y in islice(test_loader, 2):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    y = y.to(device)\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print('\\n\\n\\n')\n",
    "    print('shape of y: ', y.size())\n",
    "    print('shape of x1: ', x1.size())\n",
    "    out = net(x1, x2)\n",
    "    print('model output: ', out)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    print('Loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2406 [00:00<07:31,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2407it [07:29,  5.35it/s]                          \n",
      "  0%|          | 1/685 [00:00<01:33,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.00014746794868605878\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 685/685 [01:31<00:00,  7.48it/s]\n",
      "  0%|          | 1/2406 [00:00<07:27,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.937, macro auc 0.637 and micro auc 0.973\n",
      "time taken: 541.33\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2407it [07:24,  5.41it/s]                          \n",
      "  0%|          | 1/685 [00:00<01:30,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.00014503995077135603\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 685/685 [01:28<00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.000, accuracy 0.937, macro auc 0.636 and micro auc 0.973\n",
      "time taken: 1074.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model=net, train_dl=train_loader,\n",
    "                    valid_dl=test_loader, epochs=2,\n",
    "                    train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                    chunksize=CHUNKSIZE, batch_size=BATCH_SIZE,\n",
    "                    device=device, lr=0.02, wd=0.00001,\n",
    "                    loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.00014746794868605878,\n",
       "  'test_loss': 0.0001891156478376139,\n",
       "  'test_acc': 0.9365029674558143,\n",
       "  'test_auc_macro': 0.6369750546379287,\n",
       "  'test_auc_micro': 0.9731460712555777},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 0.00014503995077135603,\n",
       "  'test_loss': 0.0001883261116955,\n",
       "  'test_acc': 0.9365464466618839,\n",
       "  'test_auc_macro': 0.6356256690320318,\n",
       "  'test_auc_micro': 0.9726313888331596}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
