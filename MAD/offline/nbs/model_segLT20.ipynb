{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/ExternalTest/MAD/src/\")\n",
    "from constants import *\n",
    "from metadata_utils import _find_files\n",
    "from baseline_feats_utils import feat_type_feats_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx_dct = {'GE20': USER2IDX_SEGGE20_FN,\n",
    "                'LE3': USER2IDX_SEGLE3_FN,\n",
    "                '4-19': USER2IDX_SEG4TO19_FN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, sample, model_type, file_name=None,\n",
    "                 interim_data_dir=INTERIM_DATA_DIR, user_col=USER_COL,\n",
    "                 item_col=ITEM_COL, ontology_col=ONTOLOGY_COL,\n",
    "                 brand_col=BRAND_COL, price_col=PRICE_COL, dv_col=DV_COL,\n",
    "                 date_col=DATE_COL, end_token='.gz', chunksize=10,\n",
    "                 segment='LE3', user2idx_dct=user2idx_dct):\n",
    "\n",
    "        data_dir = interim_data_dir\n",
    "        \n",
    "        if file_name is None:\n",
    "            files = _find_files(data_dir, end_token)\n",
    "            if sample == 'train':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if not x.startswith('0005')]\n",
    "            elif sample == 'test':\n",
    "                self.files = [os.path.join(data_dir, x) for x in files\n",
    "                              if x.startswith('0005')]\n",
    "        else:\n",
    "            self.files = [os.path.join(data_dir, file_name)]\n",
    "        print(self.files)\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.segment = segment\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.ontology_col = ontology_col\n",
    "        self.brand_col = brand_col\n",
    "        self.price_col = price_col\n",
    "        self.date_col = date_col\n",
    "        self.dv_col = dv_col\n",
    "        self.feat_type_feats_dct = feat_type_feats_dct\n",
    "        self.chunksize = chunksize\n",
    "        user_feats = ['{}_{}'.format(self.user_col, x) for x in\n",
    "                      self.feat_type_feats_dct['user']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        user_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.user_col))\n",
    "        item_feats = ['{}_{}'.format(self.item_col, x) for x in\n",
    "                      self.feat_type_feats_dct['item']\n",
    "                      if x != 'earliest_interaction_date']\n",
    "        item_feats.append('{}_days_since_earliest_interaction'.format(\n",
    "            self.item_col))\n",
    "        self.numeric_feats = [self.price_col] + user_feats + item_feats\n",
    "        if self.segment == 'GE20':\n",
    "            self.cat_feats = [self.user_col, self.item_col,\n",
    "                              self.ontology_col, self.brand_col]\n",
    "        else:\n",
    "            self.cat_feats = [self.item_col, self.ontology_col,\n",
    "                              self.brand_col]\n",
    "        if self.segment == 'GE20':\n",
    "            self.user2idx = json.load(open(user2idx_dct.get(self.segment)))\n",
    "        else:\n",
    "            self.user2idx = None\n",
    "        \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        df = pd.read_csv(fn, compression='gzip', sep='|', iterator=True,\n",
    "                         chunksize=self.chunksize)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_dv_for_classification(self, dv_lst):\n",
    "        \n",
    "        if self.model_type == 'classification':\n",
    "            return [int(x-1) for x in dv_lst]\n",
    "        else:\n",
    "            return [int(x) for x in dv_lst]\n",
    "        \n",
    "    \n",
    "    def _segment_filter(self, num_interactions_lst, feat_type, feats_lst):\n",
    "        \n",
    "        if self.segment != 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x < 20]\n",
    "        elif self.segment == 'GE20':\n",
    "            idxs = [i for i, x in enumerate(num_interactions_lst)\n",
    "                    if x >= 20]\n",
    "        \n",
    "        if idxs:\n",
    "            new_feats_lst = [feats_lst[i] for i in idxs]\n",
    "            if (self.segment == 'GE20') and (feat_type == 'cat'):\n",
    "                new_feats_lst = []\n",
    "                for i in idxs:\n",
    "                    out = feats_lst[i]\n",
    "                    out[0] = self.user2idx[str(out[0])]\n",
    "                    new_feats_lst.append(out)\n",
    "            return new_feats_lst\n",
    "\n",
    "    \n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        for row in data:\n",
    "            num_interactions = row['uuid_num_interactions'].values.tolist()\n",
    "            \n",
    "            x1 = row[self.cat_feats].values.tolist()\n",
    "            x2 = row[self.numeric_feats].values.tolist()\n",
    "            y = self.get_dv_for_classification(\n",
    "                    row[self.dv_col].tolist())\n",
    "            x1 = self._segment_filter(num_interactions, 'cat', x1)\n",
    "            if x1:\n",
    "                x2 = self._segment_filter(num_interactions, 'numeric', x2)\n",
    "                y = self._segment_filter(num_interactions, 'dv', y)\n",
    "                yield (x1, x2, y)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductRecommendationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for product recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for\n",
    "                                         categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont, self.n_classes = n_emb, n_cont, n_classes\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 300)\n",
    "        self.lin2 = nn.Linear(300, 100)\n",
    "        self.lin3 = nn.Linear(100, self.n_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as torch_optim\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def choose_embedding_size(cat_cols, cat_num_values, min_emb_dim=100):\n",
    "    \"\"\"\n",
    "    cat_cols: list of categorical columns\n",
    "    cat_num_values: list of number of unique values for each categorical column\n",
    "    \"\"\"\n",
    "\n",
    "    embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "    embedding_sizes = [(n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "                       for _, n_categories in embedded_cols.items()]\n",
    "    return embedding_sizes\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "\n",
    "def construct_tensor(a):\n",
    "\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def train_model(model, optim, train_dl, train_size, chunksize, batch_size,\n",
    "                device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in train_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            output = model(x1, x2)\n",
    "            loss = loss_fn(output, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl, test_size, chunksize, batch_size,\n",
    "             device, loss_fn=F.cross_entropy):\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    sum_auc_macro = 0\n",
    "    sum_auc_micro = 0\n",
    "    num_aucs = 0\n",
    "    with tqdm(total=test_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in valid_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            pred_prob = F.softmax(out, dim=1)\n",
    "            y_onehot = F.one_hot(y)\n",
    "            correct += (pred == y).float().sum().item()\n",
    "            pred_prob = pred_prob.cpu().detach().numpy()\n",
    "            y_onehot = y_onehot.cpu().detach().numpy()\n",
    "            try:\n",
    "                sum_auc_macro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='macro')\n",
    "                sum_auc_micro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='micro')\n",
    "                num_aucs += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            pbar.update(1)\n",
    "    print(\"valid loss %.3f, accuracy %.3f, macro auc %.3f and micro auc %.3f\" % (\n",
    "        sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs))\n",
    "    return sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs\n",
    "\n",
    "\n",
    "def train_loop(model, train_dl, valid_dl, epochs, train_size,\n",
    "               test_size, chunksize, batch_size, device, lr=0.01,\n",
    "               wd=0.0, loss_fn=F.cross_entropy):\n",
    "\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        stats = {'epoch': i+1}\n",
    "        train_loss = train_model(model, optim, train_dl, train_size,\n",
    "                                 chunksize, batch_size, device,\n",
    "                                 loss_fn)\n",
    "        print(\"training loss: \", train_loss)\n",
    "        stats['train_loss'] = train_loss\n",
    "        loss, acc, auc_macro, auc_micro = val_loss(\n",
    "            model, valid_dl, test_size, chunksize, batch_size, device, loss_fn)\n",
    "        print('time taken: %0.2f' % (time.time() - start))\n",
    "        stats['test_loss'] = loss\n",
    "        stats['test_acc'] = acc\n",
    "        stats['test_auc_macro'] = auc_macro\n",
    "        stats['test_auc_micro'] = auc_micro\n",
    "        losses.append(stats)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "SEGMENT = 'LE3'\n",
    "N_USERS = 4881444\n",
    "N_ITEMS = 1175648\n",
    "N_ONTOLOGIES = 801\n",
    "N_BRANDS = 1686\n",
    "BATCH_SIZE = 50\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 173044425 \n",
    "TEST_SIZE = 34608886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding size\n",
    "\n",
    "if SEGMENT != 'GE20':\n",
    "    cat_cols = [ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "else:\n",
    "    cat_cols = [USER_COL, ITEM_COL, ONTOLOGY_COL, BRAND_COL]\n",
    "    cat_num_values = [N_USERS, N_ITEMS, N_ONTOLOGIES, N_BRANDS]\n",
    "\n",
    "embedding_sizes = choose_embedding_size(cat_cols, cat_num_values, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1175648, 150), (801, 150), (1686, 150)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ExternalTest_Data/MAD/interim/0000_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0003_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0001_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0000_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0002_part_04.gz', '/data/ExternalTest_Data/MAD/interim/0004_part_00.gz']\n",
      "['/data/ExternalTest_Data/MAD/interim/0005_part_05.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_02.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_07.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_00.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_01.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_03.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_06.gz', '/data/ExternalTest_Data/MAD/interim/0005_part_04.gz']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_name=None, model_type='classification',\n",
    "    sample='train', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(\n",
    "    file_name=None, model_type='classification',\n",
    "    sample='test', chunksize=CHUNKSIZE, segment=SEGMENT)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numeric vars:  18\n"
     ]
    }
   ],
   "source": [
    "n_cont = len(train_loader.dataset.numeric_feats)\n",
    "print('number of numeric vars: ', n_cont)\n",
    "\n",
    "net = ProductRecommendationModel(embedding_sizes, n_cont, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1175648, 150)\n",
       "    (1): Embedding(801, 150)\n",
       "    (2): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=468, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductRecommendationModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(1175648, 150)\n",
       "    (1): Embedding(801, 150)\n",
       "    (2): Embedding(1686, 150)\n",
       "  )\n",
       "  (lin1): Linear(in_features=468, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (bn1): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/34608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 963/34608 [04:42<2:50:12,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1238/34608 [06:02<2:42:52,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1512/34608 [07:23<2:40:16,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2448/34608 [11:56<2:39:04,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3404/34608 [16:40<2:33:45,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4367/34608 [21:23<2:29:50,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 5317/34608 [26:08<2:28:14,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 6253/34608 [30:42<2:19:43,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 7189/34608 [35:16<2:14:10,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 8137/34608 [39:57<2:11:34,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 9086/34608 [44:39<2:07:10,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 10024/34608 [49:15<2:01:59,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10980/34608 [53:56<1:55:42,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11919/34608 [58:32<1:53:20,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 12874/34608 [1:03:13<1:46:04,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 13830/34608 [1:07:55<1:41:55,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 14766/34608 [1:12:27<1:37:07,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 15714/34608 [1:17:14<1:35:58,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 15988/34608 [1:18:37<1:34:03,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 16945/34608 [1:23:22<1:26:11,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 17880/34608 [1:27:53<1:21:24,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 18831/34608 [1:32:35<1:17:50,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 19787/34608 [1:37:18<1:12:58,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 20060/34608 [1:38:39<1:11:53,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 20334/34608 [1:40:00<1:10:46,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 21290/34608 [1:44:44<1:05:47,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 22240/34608 [1:49:27<1:01:25,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 23197/34608 [1:54:15<57:27,  3.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 24136/34608 [1:58:58<52:40,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 25092/34608 [2:03:42<46:34,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 26053/34608 [2:08:22<41:27,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 26992/34608 [2:13:00<37:41,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 27931/34608 [2:17:40<34:10,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 28894/34608 [2:22:26<28:22,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 29842/34608 [2:27:12<23:51,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 30793/34608 [2:31:57<19:01,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 31741/34608 [2:36:44<14:32,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 32697/34608 [2:41:31<09:34,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 33647/34608 [2:46:23<04:54,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34610it [2:51:15,  3.37it/s]                           \n",
      "  0%|          | 0/6921 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0003241881891108636\n",
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 487/6921 [04:25<42:53,  2.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 962/6921 [08:46<38:29,  2.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1106/6921 [10:02<50:19,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1379/6921 [14:26<1:24:25,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1718/6921 [18:42<53:33,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2359/6921 [23:11<27:04,  2.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2898/6921 [27:37<36:34,  1.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3475/6921 [32:04<31:48,  1.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.001, accuracy 0.760, macro auc 0.555 and micro auc 0.915\n",
      "time taken: 12200.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model=net, train_dl=train_loader,\n",
    "                    valid_dl=test_loader, epochs=1,\n",
    "                    train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                    chunksize=CHUNKSIZE, batch_size=BATCH_SIZE,\n",
    "                    device=device, lr=0.01, wd=0.00001,\n",
    "                    loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.0003241881891108636,\n",
       "  'test_loss': 0.0006583050841074019,\n",
       "  'test_acc': 0.7603940942837969,\n",
       "  'test_auc_macro': 0.5552194619585742,\n",
       "  'test_auc_micro': 0.9154460897537869}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/py36/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ProductRecommendationModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model's state dict\n",
    "model_fn = os.path.join(MODEL_DIR, 'Class_model_SegLT20_E1.pt')\n",
    "torch.save(net.state_dict, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and checkpoint model\n",
    "model_ckpt_fn = os.path.join(MODEL_DIR, 'Class_model_SegLT20_E1_ckpt.pt')\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': get_optimizer(\n",
    "                net, lr = 0.01, wd = 0.00001).state_dict(),\n",
    "            'loss': losses[-1]['test_loss'],\n",
    "            'acc': losses[-1]['test_acc'],\n",
    "            'auc_macro': losses[-1]['test_auc_macro'],\n",
    "            'auc_micro': losses[-1]['test_auc_micro']\n",
    "            }, model_ckpt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n",
      "read data\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "preds = []\n",
    "pred_probs = []\n",
    "actuals = []\n",
    "actuals_onehot = []\n",
    "with torch.no_grad():\n",
    "    for x1, x2, y in test_loader:\n",
    "        x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                     construct_tensor_y(y))\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)\n",
    "        out = net(x1, x2)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        pred_prob = F.softmax(out, dim=1)\n",
    "        y_onehot = F.one_hot(y)\n",
    "        preds.append(pred)\n",
    "        pred_probs.append(pred_prob)\n",
    "        actuals.append(y)\n",
    "        actuals_onehot.append(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6922,\n",
       " 6922,\n",
       " 6922,\n",
       " 6922,\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], device='cuda:0'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(pred_probs), len(actuals), len(actuals_onehot), actuals_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = [item for sublist in preds for\n",
    "               item in sublist.cpu().detach().tolist()]\n",
    "final_pred_probs = [item for sublist in pred_probs for\n",
    "                    item in sublist.cpu().detach().tolist()]\n",
    "final_actuals = [item for sublist in actuals for\n",
    "                 item in sublist.cpu().detach().tolist()]\n",
    "final_actuals_onehot = F.one_hot(tensor(final_actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4738460,\n",
       " 4738460,\n",
       " 4738460,\n",
       " 4738460,\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]]),\n",
       " [[0.602712869644165, 0.397177129983902, 0.00011003491817973554],\n",
       "  [0.8356554508209229, 0.16395899653434753, 0.0003855570685118437],\n",
       "  [0.6895380020141602, 0.31031301617622375, 0.00014899118104949594],\n",
       "  [0.8836926221847534, 0.11550168693065643, 0.000805720395874232],\n",
       "  [0.9081259369850159, 0.09126979857683182, 0.0006043362081982195]],\n",
       " [0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(final_preds), len(final_pred_probs), len(final_actuals),\n",
    " len(final_actuals_onehot), final_actuals_onehot[:5],\n",
    " final_pred_probs[:5], final_actuals[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_probs = np.array(final_pred_probs)\n",
    "final_actuals_onehot = np.array(final_actuals_onehot)\n",
    "final_actuals = np.array(final_actuals)\n",
    "final_preds = np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4738460, 3)\n",
      "(4738460, 3)\n",
      "[6.02712870e-01 3.97177130e-01 1.10034918e-04]\n",
      "[[1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(final_pred_probs.shape)\n",
    "print(final_actuals_onehot.shape)\n",
    "print(final_pred_probs[0])\n",
    "print(final_actuals_onehot[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.760\n",
      "Test Macro AUC: 0.524\n",
      "Test Micro AUC: 0.923\n",
      "Test RMSE: 0.492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "acc = (final_actuals == final_preds).mean()\n",
    "auc_macro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='macro')\n",
    "auc_micro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='micro')\n",
    "rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                  y_pred=final_preds))\n",
    "\n",
    "print('Test Accuracy: %0.3f' % (acc))\n",
    "print('Test Macro AUC: %0.3f' % (auc_macro))\n",
    "print('Test Micro AUC: %0.3f' % (auc_micro))\n",
    "print('Test RMSE: %0.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4738460, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>actual_onehot</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.602712869644165, 0.397177129983902, 0.00011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8356554508209229, 0.16395899653434753, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6895380020141602, 0.31031301617622375, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8836926221847534, 0.11550168693065643, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9081259369850159, 0.09126979857683182, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual actual_onehot  pred_class  \\\n",
       "0       0     [1, 0, 0]           0   \n",
       "1       0     [1, 0, 0]           0   \n",
       "2       0     [1, 0, 0]           0   \n",
       "3       0     [1, 0, 0]           0   \n",
       "4       0     [1, 0, 0]           0   \n",
       "\n",
       "                                           pred_prob  \n",
       "0  [0.602712869644165, 0.397177129983902, 0.00011...  \n",
       "1  [0.8356554508209229, 0.16395899653434753, 0.00...  \n",
       "2  [0.6895380020141602, 0.31031301617622375, 0.00...  \n",
       "3  [0.8836926221847534, 0.11550168693065643, 0.00...  \n",
       "4  [0.9081259369850159, 0.09126979857683182, 0.00...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(\n",
    "    {'actual': final_actuals,\n",
    "     'actual_onehot': final_actuals_onehot.tolist(),\n",
    "     'pred_class': final_preds,\n",
    "     'pred_prob': final_pred_probs.tolist()})\n",
    "\n",
    "print(prediction_df.shape)\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fn = os.path.join(PREDICTION_DIR, 'prediction_SegLT20_E1.csv.gz')\n",
    "prediction_df.to_csv(pred_fn, compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
